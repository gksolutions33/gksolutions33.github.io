<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="description" content="Explore our collection of college projects including AI, Web Development, IoT, and more. Each project includes step-by-step instructions and source code." />
  <meta name="keywords" content="projects, college projects, AI, web development, IoT, tutorials, source code" />
  <title>Projects | GK Solutions</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;800&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css">
  <link rel="stylesheet" href="/assets/css/style.css">
</head>


<body>
  <header>
    <div class="site-title">GK SOLUTIONS</div>
    <div class="site-sub">AI ‚Ä¢ IoT ‚Ä¢ Arduino ‚Ä¢ Projects & Tutorials</div>
    <div class="site-sub">DEFEAT THE FEAR</div>
  </header>

  <nav>
    <a href="/market.html">Market Updates</a>
    <a href="/index.html">Home</a>
    <a href="/projects.html">Projects</a>
    <a href="/tutorials.html">Tutorials</a>
    <a href="/Papers.html">Papers</a>
    <a href="/contact.html">Contact</a>
    <a href="/about.html">About</a>
    <a href="/privacy.html">Privacy</a>
    <a href="/terms.html">Terms</a>
</nav>

  <main>
    <style>
    /* Project Showcase Specific Styles */
    .showcase-header {
        text-align: center;
        margin-bottom: 2rem;
        padding: 2rem 1rem;
        background: linear-gradient(135deg, rgba(0, 245, 255, 0.05), rgba(255, 45, 149, 0.05));
        border-radius: 16px;
    }

    .showcase-header h1 {
        font-size: clamp(1.8rem, 4vw, 2.8rem);
        font-weight: 800;
        margin-bottom: 0.5rem;
    }

    .showcase-header p {
        color: var(--muted);
        font-size: 1.1rem;
    }

    .stats-row {
        display: flex;
        gap: 2rem;
        justify-content: center;
        margin-top: 1.5rem;
        flex-wrap: wrap;
    }

    .stat-item {
        text-align: center;
    }

    .stat-number {
        font-size: 2rem;
        font-weight: 800;
        color: var(--accent-cyan);
    }

    .stat-label {
        font-size: 0.9rem;
        color: var(--muted);
    }

    /* Filters */
    .filters-section {
        margin-bottom: 2rem;
        padding: 1.5rem;
        background: rgba(255, 255, 255, 0.02);
        border-radius: 12px;
        border: 1px solid rgba(255, 255, 255, 0.05);
    }

    .filter-row {
        display: flex;
        gap: 1rem;
        align-items: center;
        flex-wrap: wrap;
        margin-bottom: 1rem;
    }

    .filter-label {
        font-weight: 600;
        min-width: 80px;
    }

    .filter-chips {
        display: flex;
        gap: 0.5rem;
        flex-wrap: wrap;
    }

    .filter-chip {
        padding: 0.4rem 1rem;
        border-radius: 999px;
        border: 2px solid rgba(255, 255, 255, 0.1);
        background: transparent;
        color: var(--muted);
        cursor: pointer;
        font-size: 0.9rem;
        font-weight: 600;
        transition: all 0.2s ease;
    }

    .filter-chip:hover {
        border-color: var(--accent-cyan);
        color: white;
    }

    .filter-chip.active {
        background: var(--accent-cyan);
        border-color: var(--accent-cyan);
        color: #001;
    }

    .search-box {
        width: 100%;
        max-width: 400px;
    }

    .search-input {
        width: 100%;
        padding: 0.7rem 1rem;
        border-radius: 999px;
        border: 2px solid rgba(255, 255, 255, 0.1);
        background: rgba(255, 255, 255, 0.02);
        color: white;
        font-size: 0.95rem;
        outline: none;
    }

    .search-input:focus {
        border-color: var(--accent-cyan);
        background: rgba(255, 255, 255, 0.05);
    }

    /* Project Grid */
    .projects-grid {
        display: grid;
        grid-template-columns: repeat(auto-fill, minmax(350px, 1fr));
        gap: 1.5rem;
        margin-top: 2rem;
    }

    @media (max-width: 768px) {
        .projects-grid {
            grid-template-columns: 1fr;
        }
    }

    /* Project Card */
    .project-card {
        background: linear-gradient(180deg, rgba(255, 255, 255, 0.03), rgba(255, 255, 255, 0.01));
        border: 2px solid rgba(255, 255, 255, 0.05);
        border-radius: 16px;
        padding: 1.5rem;
        transition: all 0.3s ease;
        cursor: pointer;
        position: relative;
        overflow: hidden;
        min-height: 420px;
        display: flex;
        flex-direction: column;
    }

    .project-card::before {
        content: '';
        position: absolute;
        top: 0;
        left: 0;
        right: 0;
        height: 4px;
        background: linear-gradient(90deg, var(--accent-cyan), var(--accent-pink));
        opacity: 0;
        transition: opacity 0.3s ease;
    }

    .project-card:hover {
        transform: translateY(-8px);
        border-color: rgba(0, 245, 255, 0.3);
        box-shadow: 0 20px 40px rgba(0, 245, 255, 0.1);
    }

    .project-card:hover::before {
        opacity: 1;
    }

    .project-header {
        display: flex;
        justify-content: space-between;
        align-items: flex-start;
        margin-bottom: 1rem;
    }

    .project-title {
        font-size: 1.3rem;
        font-weight: 700;
        margin-bottom: 0.5rem;
    }

    .difficulty-badge {
        padding: 0.3rem 0.8rem;
        border-radius: 999px;
        font-size: 0.75rem;
        font-weight: 700;
        text-transform: uppercase;
    }

    .difficulty-beginner {
        background: rgba(34, 197, 94, 0.2);
        color: #4ade80;
        border: 1px solid rgba(34, 197, 94, 0.3);
    }

    .difficulty-intermediate {
        background: rgba(251, 191, 36, 0.2);
        color: #fbbf24;
        border: 1px solid rgba(251, 191, 36, 0.3);
    }

    .difficulty-advanced {
        background: rgba(239, 68, 68, 0.2);
        color: #f87171;
        border: 1px solid rgba(239, 68, 68, 0.3);
    }

    .project-description {
        color: var(--muted);
        font-size: 0.95rem;
        line-height: 1.6;
        margin-bottom: 1rem;
        min-height: 60px;
        max-height: 60px;
        overflow: hidden;
        display: -webkit-box;
        -webkit-line-clamp: 3;
        -webkit-box-orient: vertical;
    }

    .tech-tags {
        display: flex;
        gap: 0.5rem;
        flex-wrap: wrap;
        margin-bottom: 1rem;
        min-height: 70px;
    }

    .tech-tag {
        padding: 0.3rem 0.7rem;
        background: rgba(0, 245, 255, 0.1);
        border: 1px solid rgba(0, 245, 255, 0.2);
        border-radius: 6px;
        font-size: 0.8rem;
        color: var(--accent-cyan);
        font-weight: 600;
    }

    .project-meta {
        display: flex;
        gap: 1.5rem;
        font-size: 0.85rem;
        color: var(--muted);
        margin-bottom: 1rem;
    }

    .meta-item {
        display: flex;
        align-items: center;
        gap: 0.3rem;
    }

    .project-actions {
        display: flex;
        gap: 0.8rem;
        margin-top: auto;
    }

    .action-btn {
        flex: 1;
        padding: 0.6rem 1rem;
        border-radius: 8px;
        font-weight: 700;
        font-size: 0.9rem;
        text-decoration: none;
        text-align: center;
        transition: all 0.2s ease;
    }

    .btn-primary {
        background: linear-gradient(90deg, var(--accent-cyan), var(--accent-pink));
        color: #001;
        border: none;
    }

    .btn-primary:hover {
        box-shadow: 0 0 20px rgba(0, 245, 255, 0.4);
        transform: translateY(-2px);
    }

    .btn-secondary {
        background: transparent;
        border: 2px solid rgba(255, 255, 255, 0.1);
        color: var(--muted);
    }

    .btn-secondary:hover {
        border-color: rgba(255, 255, 255, 0.3);
        color: white;
    }

    .no-results {
        text-align: center;
        padding: 3rem;
        color: var(--muted);
    }

    .no-results h3 {
        margin-bottom: 0.5rem;
    }
</style>

<div class="showcase-header">
    <h1>üöÄ Our Projects</h1>
    <p>Explore our collection of college projects with step-by-step guides</p>

    <div class="stats-row">
        <div class="stat-item">
            <div class="stat-number" id="totalProjects">29</div>
            <div class="stat-label">Projects</div>
        </div>
        <div class="stat-item">
            <div class="stat-number" id="totalLanguages">-</div>
            <div class="stat-label">Technologies</div>
        </div>
        <div class="stat-item">
            <div class="stat-number" id="totalStars">-</div>
            <div class="stat-label">Stars</div>
        </div>
    </div>
</div>

<div class="filters-section">
    <div class="filter-row">
        <div class="filter-label">Difficulty:</div>
        <div class="filter-chips" id="difficultyFilters">
            <button class="filter-chip active" data-filter="all">All</button>
            <button class="filter-chip" data-filter="beginner">Beginner</button>
            <button class="filter-chip" data-filter="intermediate">Intermediate</button>
            <button class="filter-chip" data-filter="advanced">Advanced</button>
        </div>
    </div>

    <div class="filter-row">
        <div class="filter-label">Technology:</div>
        <div class="filter-chips" id="techFilters">
            <button class="filter-chip active" data-filter="all">All</button>
            <!-- Dynamic tech filters will be added by JavaScript -->
        </div>
    </div>

    <div class="filter-row">
        <div class="filter-label">Search:</div>
        <div class="search-box">
            <input type="text" id="searchInput" class="search-input" placeholder="Search projects...">
        </div>
    </div>
</div>

<div class="projects-grid" id="projectsGrid">
    
    <div class="project-card" data-difficulty="intermediate"
        data-languages="Python" data-technologies="NumPy,Python"
        data-name="driver-dowsiness" data-description="no description available">

        <div class="project-header">
            <div>
                <h3 class="project-title">Driver Dowsiness</h3>
            </div>
            <span class="difficulty-badge difficulty-intermediate">intermediate</span>
        </div>

        <p class="project-description">No description available</p>

        
        <div class="tech-tags">
            
            <span class="tech-tag">Python</span>
            
            
            <span class="tech-tag">NumPy</span>
            
            <span class="tech-tag">Python</span>
            
        </div>
        

        <div class="project-meta">
            <div class="meta-item">
                <span>‚≠ê</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>üç¥</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>‚è±Ô∏è</span>
                <span>Varies</span>
            </div>
        </div>

        <div class="project-actions">
            <a href="https://github.com/Universal-college-projects/driver-dowsiness" target="_blank" class="action-btn btn-primary">
                View on GitHub ‚Üí
            </a>
            <a href="project-1.html" class="action-btn btn-secondary">
                Details
            </a>
        </div>
    </div>
    
    <div class="project-card" data-difficulty="beginner"
        data-languages="Python,Cython,C,XSLT,C++" data-technologies="Fortran,JavaScript,Jupyter Notebook,Roff,Jinja,C,Meson,XSLT,Shell,React"
        data-name="image-and-text-plagiarism-detection" data-description="no description available">

        <div class="project-header">
            <div>
                <h3 class="project-title">Image And Text Plagiarism Detection</h3>
            </div>
            <span class="difficulty-badge difficulty-beginner">beginner</span>
        </div>

        <p class="project-description">No description available</p>

        
        <div class="tech-tags">
            
            <span class="tech-tag">Python</span>
            
            <span class="tech-tag">Cython</span>
            
            <span class="tech-tag">C</span>
            
            
            <span class="tech-tag">Fortran</span>
            
            <span class="tech-tag">JavaScript</span>
            
        </div>
        

        <div class="project-meta">
            <div class="meta-item">
                <span>‚≠ê</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>üç¥</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>‚è±Ô∏è</span>
                <span>Varies</span>
            </div>
        </div>

        <div class="project-actions">
            <a href="https://github.com/Universal-college-projects/Image-and-text-plagiarism-detection" target="_blank" class="action-btn btn-primary">
                View on GitHub ‚Üí
            </a>
            <a href="project-2.html" class="action-btn btn-secondary">
                Details
            </a>
        </div>
    </div>
    
    <div class="project-card" data-difficulty="intermediate"
        data-languages="Python,CSS,JavaScript,HTML" data-technologies="CSS,JavaScript,Python,HTML"
        data-name="cyber-threats" data-description="no description available">

        <div class="project-header">
            <div>
                <h3 class="project-title">Cyber Threats</h3>
            </div>
            <span class="difficulty-badge difficulty-intermediate">intermediate</span>
        </div>

        <p class="project-description">No description available</p>

        
        <div class="tech-tags">
            
            <span class="tech-tag">Python</span>
            
            <span class="tech-tag">CSS</span>
            
            <span class="tech-tag">JavaScript</span>
            
            
            <span class="tech-tag">CSS</span>
            
            <span class="tech-tag">JavaScript</span>
            
        </div>
        

        <div class="project-meta">
            <div class="meta-item">
                <span>‚≠ê</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>üç¥</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>‚è±Ô∏è</span>
                <span>Varies</span>
            </div>
        </div>

        <div class="project-actions">
            <a href="https://github.com/Universal-college-projects/cyber-threats" target="_blank" class="action-btn btn-primary">
                View on GitHub ‚Üí
            </a>
            <a href="project-3.html" class="action-btn btn-secondary">
                Details
            </a>
        </div>
    </div>
    
    <div class="project-card" data-difficulty="advanced"
        data-languages="Python,HTML" data-technologies="Python,scikit-learn,FastAPI,pandas,Flask,HTML,Raspberry Pi,NumPy"
        data-name="solar-panel-detection-clasification" data-description="no description available">

        <div class="project-header">
            <div>
                <h3 class="project-title">Solar Panel Detection Clasification</h3>
            </div>
            <span class="difficulty-badge difficulty-advanced">advanced</span>
        </div>

        <p class="project-description">No description available</p>

        
        <div class="tech-tags">
            
            <span class="tech-tag">Python</span>
            
            <span class="tech-tag">HTML</span>
            
            
            <span class="tech-tag">Python</span>
            
            <span class="tech-tag">scikit-learn</span>
            
        </div>
        

        <div class="project-meta">
            <div class="meta-item">
                <span>‚≠ê</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>üç¥</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>‚è±Ô∏è</span>
                <span>Varies</span>
            </div>
        </div>

        <div class="project-actions">
            <a href="https://github.com/Universal-college-projects/solar-panel-detection-clasification" target="_blank" class="action-btn btn-primary">
                View on GitHub ‚Üí
            </a>
            <a href="project-4.html" class="action-btn btn-secondary">
                Details
            </a>
        </div>
    </div>
    
    <div class="project-card" data-difficulty="beginner"
        data-languages="HTML,Python" data-technologies="Python,Keras,Flask,TensorFlow,HTML,NumPy"
        data-name="automated-pest-disease-detection-for-agriculture" data-description="no description available">

        <div class="project-header">
            <div>
                <h3 class="project-title">Automated Pest Disease Detection For Agriculture</h3>
            </div>
            <span class="difficulty-badge difficulty-beginner">beginner</span>
        </div>

        <p class="project-description">No description available</p>

        
        <div class="tech-tags">
            
            <span class="tech-tag">HTML</span>
            
            <span class="tech-tag">Python</span>
            
            
            <span class="tech-tag">Python</span>
            
            <span class="tech-tag">Keras</span>
            
        </div>
        

        <div class="project-meta">
            <div class="meta-item">
                <span>‚≠ê</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>üç¥</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>‚è±Ô∏è</span>
                <span>Varies</span>
            </div>
        </div>

        <div class="project-actions">
            <a href="https://github.com/Universal-college-projects/Automated-pest-disease-detection-for-agriculture" target="_blank" class="action-btn btn-primary">
                View on GitHub ‚Üí
            </a>
            <a href="project-5.html" class="action-btn btn-secondary">
                Details
            </a>
        </div>
    </div>
    
    <div class="project-card" data-difficulty="beginner"
        data-languages="Java" data-technologies="Java,Express"
        data-name="grama-soil-express-app" data-description="no description available">

        <div class="project-header">
            <div>
                <h3 class="project-title">Grama Soil Express App</h3>
            </div>
            <span class="difficulty-badge difficulty-beginner">beginner</span>
        </div>

        <p class="project-description">No description available</p>

        
        <div class="tech-tags">
            
            <span class="tech-tag">Java</span>
            
            
            <span class="tech-tag">Java</span>
            
            <span class="tech-tag">Express</span>
            
        </div>
        

        <div class="project-meta">
            <div class="meta-item">
                <span>‚≠ê</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>üç¥</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>‚è±Ô∏è</span>
                <span>Varies</span>
            </div>
        </div>

        <div class="project-actions">
            <a href="https://github.com/Universal-college-projects/grama-soil-express-app" target="_blank" class="action-btn btn-primary">
                View on GitHub ‚Üí
            </a>
            <a href="project-6.html" class="action-btn btn-secondary">
                Details
            </a>
        </div>
    </div>
    
    <div class="project-card" data-difficulty="intermediate"
        data-languages="HTML,JavaScript,CSS" data-technologies="CSS,HTML,JavaScript"
        data-name="real-time-learning-assistant" data-description="no description available">

        <div class="project-header">
            <div>
                <h3 class="project-title">Real Time Learning Assistant</h3>
            </div>
            <span class="difficulty-badge difficulty-intermediate">intermediate</span>
        </div>

        <p class="project-description">No description available</p>

        
        <div class="tech-tags">
            
            <span class="tech-tag">HTML</span>
            
            <span class="tech-tag">JavaScript</span>
            
            <span class="tech-tag">CSS</span>
            
            
            <span class="tech-tag">CSS</span>
            
            <span class="tech-tag">HTML</span>
            
        </div>
        

        <div class="project-meta">
            <div class="meta-item">
                <span>‚≠ê</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>üç¥</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>‚è±Ô∏è</span>
                <span>Varies</span>
            </div>
        </div>

        <div class="project-actions">
            <a href="https://github.com/Universal-college-projects/real-time-learning-assistant" target="_blank" class="action-btn btn-primary">
                View on GitHub ‚Üí
            </a>
            <a href="project-7.html" class="action-btn btn-secondary">
                Details
            </a>
        </div>
    </div>
    
    <div class="project-card" data-difficulty="intermediate"
        data-languages="Python" data-technologies="Flask,Python"
        data-name="comprehensive-automated-doc-verification" data-description="no description available">

        <div class="project-header">
            <div>
                <h3 class="project-title">Comprehensive Automated Doc Verification</h3>
            </div>
            <span class="difficulty-badge difficulty-intermediate">intermediate</span>
        </div>

        <p class="project-description">No description available</p>

        
        <div class="tech-tags">
            
            <span class="tech-tag">Python</span>
            
            
            <span class="tech-tag">Flask</span>
            
            <span class="tech-tag">Python</span>
            
        </div>
        

        <div class="project-meta">
            <div class="meta-item">
                <span>‚≠ê</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>üç¥</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>‚è±Ô∏è</span>
                <span>Varies</span>
            </div>
        </div>

        <div class="project-actions">
            <a href="https://github.com/Universal-college-projects/comprehensive-automated-doc-verification" target="_blank" class="action-btn btn-primary">
                View on GitHub ‚Üí
            </a>
            <a href="project-8.html" class="action-btn btn-secondary">
                Details
            </a>
        </div>
    </div>
    
    <div class="project-card" data-difficulty="advanced"
        data-languages="Jupyter Notebook,Python" data-technologies="Python,Jupyter Notebook"
        data-name="deep-learning-techniques-for-diabetic-retinopathy-classification-" data-description="no description available">

        <div class="project-header">
            <div>
                <h3 class="project-title">Deep Learning Techniques For Diabetic Retinopathy Classification </h3>
            </div>
            <span class="difficulty-badge difficulty-advanced">advanced</span>
        </div>

        <p class="project-description">No description available</p>

        
        <div class="tech-tags">
            
            <span class="tech-tag">Jupyter Notebook</span>
            
            <span class="tech-tag">Python</span>
            
            
            <span class="tech-tag">Python</span>
            
            <span class="tech-tag">Jupyter Notebook</span>
            
        </div>
        

        <div class="project-meta">
            <div class="meta-item">
                <span>‚≠ê</span>
                <span>1</span>
            </div>
            <div class="meta-item">
                <span>üç¥</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>‚è±Ô∏è</span>
                <span>Varies</span>
            </div>
        </div>

        <div class="project-actions">
            <a href="https://github.com/Universal-college-projects/Deep-Learning-Techniques-for-Diabetic-Retinopathy-classification-" target="_blank" class="action-btn btn-primary">
                View on GitHub ‚Üí
            </a>
            <a href="project-9.html" class="action-btn btn-secondary">
                Details
            </a>
        </div>
    </div>
    
    <div class="project-card" data-difficulty="advanced"
        data-languages="Python" data-technologies="TensorFlow,NumPy,Flask,Python"
        data-name="audio-data-preparation-and-argument-using-tensorflow-" data-description="no description available">

        <div class="project-header">
            <div>
                <h3 class="project-title">Audio Data Preparation And Argument Using Tensorflow </h3>
            </div>
            <span class="difficulty-badge difficulty-advanced">advanced</span>
        </div>

        <p class="project-description">No description available</p>

        
        <div class="tech-tags">
            
            <span class="tech-tag">Python</span>
            
            
            <span class="tech-tag">TensorFlow</span>
            
            <span class="tech-tag">NumPy</span>
            
        </div>
        

        <div class="project-meta">
            <div class="meta-item">
                <span>‚≠ê</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>üç¥</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>‚è±Ô∏è</span>
                <span>Varies</span>
            </div>
        </div>

        <div class="project-actions">
            <a href="https://github.com/Universal-college-projects/Audio-data-preparation-and-argument-using-tensorflow-" target="_blank" class="action-btn btn-primary">
                View on GitHub ‚Üí
            </a>
            <a href="project-10.html" class="action-btn btn-secondary">
                Details
            </a>
        </div>
    </div>
    
    <div class="project-card" data-difficulty="beginner"
        data-languages="Python" data-technologies="Arduino,IoT,Python,ESP32"
        data-name="patient-health-monitoring" data-description="no description available">

        <div class="project-header">
            <div>
                <h3 class="project-title">Patient Health Monitoring</h3>
            </div>
            <span class="difficulty-badge difficulty-beginner">beginner</span>
        </div>

        <p class="project-description">No description available</p>

        
        <div class="tech-tags">
            
            <span class="tech-tag">Python</span>
            
            
            <span class="tech-tag">Arduino</span>
            
            <span class="tech-tag">IoT</span>
            
        </div>
        

        <div class="project-meta">
            <div class="meta-item">
                <span>‚≠ê</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>üç¥</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>‚è±Ô∏è</span>
                <span>Varies</span>
            </div>
        </div>

        <div class="project-actions">
            <a href="https://github.com/Universal-college-projects/patient-health-monitoring" target="_blank" class="action-btn btn-primary">
                View on GitHub ‚Üí
            </a>
            <a href="project-11.html" class="action-btn btn-secondary">
                Details
            </a>
        </div>
    </div>
    
    <div class="project-card" data-difficulty="intermediate"
        data-languages="" data-technologies="IoT,ESP32,Arduino,Raspberry Pi"
        data-name="smart-parking-" data-description="no description available">

        <div class="project-header">
            <div>
                <h3 class="project-title">Smart Parking </h3>
            </div>
            <span class="difficulty-badge difficulty-intermediate">intermediate</span>
        </div>

        <p class="project-description">No description available</p>

        
        <div class="tech-tags">
            
            
            <span class="tech-tag">IoT</span>
            
            <span class="tech-tag">ESP32</span>
            
        </div>
        

        <div class="project-meta">
            <div class="meta-item">
                <span>‚≠ê</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>üç¥</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>‚è±Ô∏è</span>
                <span>Varies</span>
            </div>
        </div>

        <div class="project-actions">
            <a href="https://github.com/Universal-college-projects/smart-parking-" target="_blank" class="action-btn btn-primary">
                View on GitHub ‚Üí
            </a>
            <a href="project-12.html" class="action-btn btn-secondary">
                Details
            </a>
        </div>
    </div>
    
    <div class="project-card" data-difficulty="intermediate"
        data-languages="Jupyter Notebook,Python" data-technologies="Python,Jupyter Notebook"
        data-name="rag-knowledge-chat-boot" data-description="no description available">

        <div class="project-header">
            <div>
                <h3 class="project-title">Rag Knowledge Chat Boot</h3>
            </div>
            <span class="difficulty-badge difficulty-intermediate">intermediate</span>
        </div>

        <p class="project-description">No description available</p>

        
        <div class="tech-tags">
            
            <span class="tech-tag">Jupyter Notebook</span>
            
            <span class="tech-tag">Python</span>
            
            
            <span class="tech-tag">Python</span>
            
            <span class="tech-tag">Jupyter Notebook</span>
            
        </div>
        

        <div class="project-meta">
            <div class="meta-item">
                <span>‚≠ê</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>üç¥</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>‚è±Ô∏è</span>
                <span>Varies</span>
            </div>
        </div>

        <div class="project-actions">
            <a href="https://github.com/Universal-college-projects/Rag-knowledge-chat-boot" target="_blank" class="action-btn btn-primary">
                View on GitHub ‚Üí
            </a>
            <a href="project-13.html" class="action-btn btn-secondary">
                Details
            </a>
        </div>
    </div>
    
    <div class="project-card" data-difficulty="intermediate"
        data-languages="" data-technologies=""
        data-name="ai-based-hr-screening-chatbot" data-description="no description available">

        <div class="project-header">
            <div>
                <h3 class="project-title">Ai Based Hr Screening Chatbot</h3>
            </div>
            <span class="difficulty-badge difficulty-intermediate">intermediate</span>
        </div>

        <p class="project-description">No description available</p>

        

        <div class="project-meta">
            <div class="meta-item">
                <span>‚≠ê</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>üç¥</span>
                <span>1</span>
            </div>
            <div class="meta-item">
                <span>‚è±Ô∏è</span>
                <span>Varies</span>
            </div>
        </div>

        <div class="project-actions">
            <a href="https://github.com/Universal-college-projects/Ai-based-hr-screening-chatbot" target="_blank" class="action-btn btn-primary">
                View on GitHub ‚Üí
            </a>
            <a href="project-14.html" class="action-btn btn-secondary">
                Details
            </a>
        </div>
    </div>
    
    <div class="project-card" data-difficulty="beginner"
        data-languages="HTML,Python" data-technologies="Python,scikit-learn,pandas,Flask,HTML,NumPy"
        data-name="heart-disease-detection-naive-bayes-" data-description="no description available">

        <div class="project-header">
            <div>
                <h3 class="project-title">Heart Disease Detection Naive Bayes </h3>
            </div>
            <span class="difficulty-badge difficulty-beginner">beginner</span>
        </div>

        <p class="project-description">No description available</p>

        
        <div class="tech-tags">
            
            <span class="tech-tag">HTML</span>
            
            <span class="tech-tag">Python</span>
            
            
            <span class="tech-tag">Python</span>
            
            <span class="tech-tag">scikit-learn</span>
            
        </div>
        

        <div class="project-meta">
            <div class="meta-item">
                <span>‚≠ê</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>üç¥</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>‚è±Ô∏è</span>
                <span>Varies</span>
            </div>
        </div>

        <div class="project-actions">
            <a href="https://github.com/Universal-college-projects/Heart-disease-detection-naive-bayes-" target="_blank" class="action-btn btn-primary">
                View on GitHub ‚Üí
            </a>
            <a href="project-15.html" class="action-btn btn-secondary">
                Details
            </a>
        </div>
    </div>
    
    <div class="project-card" data-difficulty="intermediate"
        data-languages="Python" data-technologies="Python"
        data-name="automatic-traffic-signal-control-using-ai" data-description="no description available">

        <div class="project-header">
            <div>
                <h3 class="project-title">Automatic Traffic Signal Control Using Ai</h3>
            </div>
            <span class="difficulty-badge difficulty-intermediate">intermediate</span>
        </div>

        <p class="project-description">No description available</p>

        
        <div class="tech-tags">
            
            <span class="tech-tag">Python</span>
            
            
            <span class="tech-tag">Python</span>
            
        </div>
        

        <div class="project-meta">
            <div class="meta-item">
                <span>‚≠ê</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>üç¥</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>‚è±Ô∏è</span>
                <span>Varies</span>
            </div>
        </div>

        <div class="project-actions">
            <a href="https://github.com/Universal-college-projects/Automatic-traffic-signal-control-using-AI" target="_blank" class="action-btn btn-primary">
                View on GitHub ‚Üí
            </a>
            <a href="project-16.html" class="action-btn btn-secondary">
                Details
            </a>
        </div>
    </div>
    
    <div class="project-card" data-difficulty="intermediate"
        data-languages="" data-technologies=""
        data-name="go-grocery-serve-app" data-description="no description available">

        <div class="project-header">
            <div>
                <h3 class="project-title">Go Grocery Serve App</h3>
            </div>
            <span class="difficulty-badge difficulty-intermediate">intermediate</span>
        </div>

        <p class="project-description">No description available</p>

        

        <div class="project-meta">
            <div class="meta-item">
                <span>‚≠ê</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>üç¥</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>‚è±Ô∏è</span>
                <span>Varies</span>
            </div>
        </div>

        <div class="project-actions">
            <a href="https://github.com/Universal-college-projects/go-grocery-serve-app" target="_blank" class="action-btn btn-primary">
                View on GitHub ‚Üí
            </a>
            <a href="project-17.html" class="action-btn btn-secondary">
                Details
            </a>
        </div>
    </div>
    
    <div class="project-card" data-difficulty="beginner"
        data-languages="Python,CSS,JavaScript,HTML" data-technologies="JavaScript,Python,Keras,Flask,TensorFlow,HTML,CSS,NumPy"
        data-name="skin-disease-detection-and-classification-using-cnn" data-description="no description available">

        <div class="project-header">
            <div>
                <h3 class="project-title">Skin Disease Detection And Classification Using Cnn</h3>
            </div>
            <span class="difficulty-badge difficulty-beginner">beginner</span>
        </div>

        <p class="project-description">No description available</p>

        
        <div class="tech-tags">
            
            <span class="tech-tag">Python</span>
            
            <span class="tech-tag">CSS</span>
            
            <span class="tech-tag">JavaScript</span>
            
            
            <span class="tech-tag">JavaScript</span>
            
            <span class="tech-tag">Python</span>
            
        </div>
        

        <div class="project-meta">
            <div class="meta-item">
                <span>‚≠ê</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>üç¥</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>‚è±Ô∏è</span>
                <span>Varies</span>
            </div>
        </div>

        <div class="project-actions">
            <a href="https://github.com/Universal-college-projects/Skin-disease-detection-and-classification-using-cnn" target="_blank" class="action-btn btn-primary">
                View on GitHub ‚Üí
            </a>
            <a href="project-18.html" class="action-btn btn-secondary">
                Details
            </a>
        </div>
    </div>
    
    <div class="project-card" data-difficulty="intermediate"
        data-languages="" data-technologies=""
        data-name="autonomous-drone" data-description="no description available">

        <div class="project-header">
            <div>
                <h3 class="project-title">Autonomous Drone</h3>
            </div>
            <span class="difficulty-badge difficulty-intermediate">intermediate</span>
        </div>

        <p class="project-description">No description available</p>

        

        <div class="project-meta">
            <div class="meta-item">
                <span>‚≠ê</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>üç¥</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>‚è±Ô∏è</span>
                <span>Varies</span>
            </div>
        </div>

        <div class="project-actions">
            <a href="https://github.com/Universal-college-projects/autonomous-drone" target="_blank" class="action-btn btn-primary">
                View on GitHub ‚Üí
            </a>
            <a href="project-19.html" class="action-btn btn-secondary">
                Details
            </a>
        </div>
    </div>
    
    <div class="project-card" data-difficulty="intermediate"
        data-languages="Python" data-technologies="Python"
        data-name="road-condition-monitoring" data-description="no description available">

        <div class="project-header">
            <div>
                <h3 class="project-title">Road Condition Monitoring</h3>
            </div>
            <span class="difficulty-badge difficulty-intermediate">intermediate</span>
        </div>

        <p class="project-description">No description available</p>

        
        <div class="tech-tags">
            
            <span class="tech-tag">Python</span>
            
            
            <span class="tech-tag">Python</span>
            
        </div>
        

        <div class="project-meta">
            <div class="meta-item">
                <span>‚≠ê</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>üç¥</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>‚è±Ô∏è</span>
                <span>Varies</span>
            </div>
        </div>

        <div class="project-actions">
            <a href="https://github.com/Universal-college-projects/road-condition-monitoring" target="_blank" class="action-btn btn-primary">
                View on GitHub ‚Üí
            </a>
            <a href="project-20.html" class="action-btn btn-secondary">
                Details
            </a>
        </div>
    </div>
    
    <div class="project-card" data-difficulty="intermediate"
        data-languages="JavaScript,HTML,Solidity,CSS" data-technologies="CSS,JavaScript,HTML,Solidity"
        data-name="blockchain" data-description="no description available">

        <div class="project-header">
            <div>
                <h3 class="project-title">Blockchain</h3>
            </div>
            <span class="difficulty-badge difficulty-intermediate">intermediate</span>
        </div>

        <p class="project-description">No description available</p>

        
        <div class="tech-tags">
            
            <span class="tech-tag">JavaScript</span>
            
            <span class="tech-tag">HTML</span>
            
            <span class="tech-tag">Solidity</span>
            
            
            <span class="tech-tag">CSS</span>
            
            <span class="tech-tag">JavaScript</span>
            
        </div>
        

        <div class="project-meta">
            <div class="meta-item">
                <span>‚≠ê</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>üç¥</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>‚è±Ô∏è</span>
                <span>Varies</span>
            </div>
        </div>

        <div class="project-actions">
            <a href="https://github.com/Universal-college-projects/blockchain" target="_blank" class="action-btn btn-primary">
                View on GitHub ‚Üí
            </a>
            <a href="project-21.html" class="action-btn btn-secondary">
                Details
            </a>
        </div>
    </div>
    
    <div class="project-card" data-difficulty="intermediate"
        data-languages="Python,Batchfile" data-technologies="Batchfile,Python,Express"
        data-name="emotion-detection-from-speech-and-facial-expression-" data-description="no description available">

        <div class="project-header">
            <div>
                <h3 class="project-title">Emotion Detection From Speech And Facial Expression </h3>
            </div>
            <span class="difficulty-badge difficulty-intermediate">intermediate</span>
        </div>

        <p class="project-description">No description available</p>

        
        <div class="tech-tags">
            
            <span class="tech-tag">Python</span>
            
            <span class="tech-tag">Batchfile</span>
            
            
            <span class="tech-tag">Batchfile</span>
            
            <span class="tech-tag">Python</span>
            
        </div>
        

        <div class="project-meta">
            <div class="meta-item">
                <span>‚≠ê</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>üç¥</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>‚è±Ô∏è</span>
                <span>Varies</span>
            </div>
        </div>

        <div class="project-actions">
            <a href="https://github.com/Universal-college-projects/Emotion-detection-from-speech-and-facial-expression-" target="_blank" class="action-btn btn-primary">
                View on GitHub ‚Üí
            </a>
            <a href="project-22.html" class="action-btn btn-secondary">
                Details
            </a>
        </div>
    </div>
    
    <div class="project-card" data-difficulty="intermediate"
        data-languages="" data-technologies=""
        data-name="my-personal-chef" data-description="no description available">

        <div class="project-header">
            <div>
                <h3 class="project-title">My Personal Chef</h3>
            </div>
            <span class="difficulty-badge difficulty-intermediate">intermediate</span>
        </div>

        <p class="project-description">No description available</p>

        

        <div class="project-meta">
            <div class="meta-item">
                <span>‚≠ê</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>üç¥</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>‚è±Ô∏è</span>
                <span>Varies</span>
            </div>
        </div>

        <div class="project-actions">
            <a href="https://github.com/Universal-college-projects/my-personal-chef" target="_blank" class="action-btn btn-primary">
                View on GitHub ‚Üí
            </a>
            <a href="project-23.html" class="action-btn btn-secondary">
                Details
            </a>
        </div>
    </div>
    
    <div class="project-card" data-difficulty="intermediate"
        data-languages="" data-technologies="IoT"
        data-name="vitamin-deficiency" data-description="no description available">

        <div class="project-header">
            <div>
                <h3 class="project-title">Vitamin Deficiency</h3>
            </div>
            <span class="difficulty-badge difficulty-intermediate">intermediate</span>
        </div>

        <p class="project-description">No description available</p>

        
        <div class="tech-tags">
            
            
            <span class="tech-tag">IoT</span>
            
        </div>
        

        <div class="project-meta">
            <div class="meta-item">
                <span>‚≠ê</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>üç¥</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>‚è±Ô∏è</span>
                <span>20 mins</span>
            </div>
        </div>

        <div class="project-actions">
            <a href="https://github.com/Universal-college-projects/vitamin-deficiency" target="_blank" class="action-btn btn-primary">
                View on GitHub ‚Üí
            </a>
            <a href="project-24.html" class="action-btn btn-secondary">
                Details
            </a>
        </div>
    </div>
    
    <div class="project-card" data-difficulty="intermediate"
        data-languages="Python" data-technologies="NumPy,Python"
        data-name="face-detcetion-and-security-alert" data-description="no description available">

        <div class="project-header">
            <div>
                <h3 class="project-title">Face Detcetion And Security Alert</h3>
            </div>
            <span class="difficulty-badge difficulty-intermediate">intermediate</span>
        </div>

        <p class="project-description">No description available</p>

        
        <div class="tech-tags">
            
            <span class="tech-tag">Python</span>
            
            
            <span class="tech-tag">NumPy</span>
            
            <span class="tech-tag">Python</span>
            
        </div>
        

        <div class="project-meta">
            <div class="meta-item">
                <span>‚≠ê</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>üç¥</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>‚è±Ô∏è</span>
                <span>Varies</span>
            </div>
        </div>

        <div class="project-actions">
            <a href="https://github.com/Universal-college-projects/face-detcetion-and-security-alert" target="_blank" class="action-btn btn-primary">
                View on GitHub ‚Üí
            </a>
            <a href="project-25.html" class="action-btn btn-secondary">
                Details
            </a>
        </div>
    </div>
    
    <div class="project-card" data-difficulty="intermediate"
        data-languages="" data-technologies=""
        data-name="multilingual-chatbot-rasa" data-description="no description available">

        <div class="project-header">
            <div>
                <h3 class="project-title">Multilingual Chatbot Rasa</h3>
            </div>
            <span class="difficulty-badge difficulty-intermediate">intermediate</span>
        </div>

        <p class="project-description">No description available</p>

        

        <div class="project-meta">
            <div class="meta-item">
                <span>‚≠ê</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>üç¥</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>‚è±Ô∏è</span>
                <span>Varies</span>
            </div>
        </div>

        <div class="project-actions">
            <a href="https://github.com/Universal-college-projects/Multilingual-chatbot-rasa" target="_blank" class="action-btn btn-primary">
                View on GitHub ‚Üí
            </a>
            <a href="project-26.html" class="action-btn btn-secondary">
                Details
            </a>
        </div>
    </div>
    
    <div class="project-card" data-difficulty="intermediate"
        data-languages="" data-technologies=""
        data-name="breast-cancer-detection-using-cnn-" data-description="no description available">

        <div class="project-header">
            <div>
                <h3 class="project-title">Breast Cancer Detection Using Cnn </h3>
            </div>
            <span class="difficulty-badge difficulty-intermediate">intermediate</span>
        </div>

        <p class="project-description">No description available</p>

        

        <div class="project-meta">
            <div class="meta-item">
                <span>‚≠ê</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>üç¥</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>‚è±Ô∏è</span>
                <span>Varies</span>
            </div>
        </div>

        <div class="project-actions">
            <a href="https://github.com/Universal-college-projects/Breast-cancer-detection-using-cnn-" target="_blank" class="action-btn btn-primary">
                View on GitHub ‚Üí
            </a>
            <a href="project-27.html" class="action-btn btn-secondary">
                Details
            </a>
        </div>
    </div>
    
    <div class="project-card" data-difficulty="intermediate"
        data-languages="" data-technologies=""
        data-name="genetic-diseases-" data-description="no description available">

        <div class="project-header">
            <div>
                <h3 class="project-title">Genetic Diseases </h3>
            </div>
            <span class="difficulty-badge difficulty-intermediate">intermediate</span>
        </div>

        <p class="project-description">No description available</p>

        

        <div class="project-meta">
            <div class="meta-item">
                <span>‚≠ê</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>üç¥</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>‚è±Ô∏è</span>
                <span>Varies</span>
            </div>
        </div>

        <div class="project-actions">
            <a href="https://github.com/Universal-college-projects/Genetic-diseases-" target="_blank" class="action-btn btn-primary">
                View on GitHub ‚Üí
            </a>
            <a href="project-28.html" class="action-btn btn-secondary">
                Details
            </a>
        </div>
    </div>
    
    <div class="project-card" data-difficulty="intermediate"
        data-languages="" data-technologies=""
        data-name="fine-tuning-llm" data-description="no description available">

        <div class="project-header">
            <div>
                <h3 class="project-title">Fine Tuning Llm</h3>
            </div>
            <span class="difficulty-badge difficulty-intermediate">intermediate</span>
        </div>

        <p class="project-description">No description available</p>

        

        <div class="project-meta">
            <div class="meta-item">
                <span>‚≠ê</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>üç¥</span>
                <span>0</span>
            </div>
            <div class="meta-item">
                <span>‚è±Ô∏è</span>
                <span>Varies</span>
            </div>
        </div>

        <div class="project-actions">
            <a href="https://github.com/Universal-college-projects/Fine-tuning-llm" target="_blank" class="action-btn btn-primary">
                View on GitHub ‚Üí
            </a>
            <a href="project-29.html" class="action-btn btn-secondary">
                Details
            </a>
        </div>
    </div>
    
</div>

<div class="no-results hidden" id="noResults">
    <h3>No projects found</h3>
    <p>Try adjusting your filters or search terms</p>
</div>

<script>
    // Get all unique technologies
    const projectsData = [{"name":"driver-dowsiness","department":"AI","title":"Driver Dowsiness","description":"No description available","repo_url":"https://github.com/Universal-college-projects/driver-dowsiness","demo_url":null,"languages":["Python"],"topics":[],"difficulty":"intermediate","estimated_time":"Varies","readme_content":"# Driver Drowsiness Detection System\r\n\r\nDriver Drowsiness Detection is a real-time computer vision project developed using Python, OpenCV, and Dlib. The system continuously monitors the driver‚Äôs eye movements using facial landmarks and provides visual and audio alerts when drowsiness or sleep is detected.\r\n\r\n## Project Logic\r\nThis project uses Dlib‚Äôs 68 facial landmark detector along with OpenCV for real-time video processing. Using eye landmarks, the Eye Aspect Ratio (EAR) is calculated to determine the driver‚Äôs alertness level.\r\n\r\n## Eye Aspect Ratio (EAR)\r\nEAR is calculated using the ratio of vertical eye distances to horizontal eye distance. A lower EAR indicates closed eyes.\r\n\r\n## Working of the Project\r\n- Webcam captures live video\r\n- Face and facial landmarks are detected\r\n- EAR is computed in real time\r\n- Time-based logic detects sleeping and drowsiness\r\n- Audio alerts are triggered on state change\r\n- Driver state is logged with timestamps\r\n- Dashboard UI displays status and metrics\r\n\r\n## Technologies Used\r\nPython, OpenCV, Dlib, NumPy, Imutils, Playsound\r\n\r\n## How to Run\r\n1. Install dependencies  \r\n   `pip install opencv-python dlib imutils numpy playsound`\r\n2. Run  \r\n   `python main.py`\r\n3. Press Q or ESC to exit\r\n\r\n## Conclusion\r\nThis system helps prevent accidents by detecting driver fatigue in real time using computer vision.","setup_steps":["Install dependencies","Press Q or ESC to exit"],"technologies":["NumPy","Python"],"stars":0,"forks":0,"last_updated":"2025-12-22T06:57:59Z","created_at":"2025-12-09T10:00:29Z","default_branch":"main"},{"name":"Image-and-text-plagiarism-detection","department":"AI","title":"Image And Text Plagiarism Detection","description":"No description available","repo_url":"https://github.com/Universal-college-projects/Image-and-text-plagiarism-detection","demo_url":null,"languages":["Python","Cython","C","XSLT","C++"],"topics":[],"difficulty":"beginner","estimated_time":"Varies","readme_content":"# Image-and-text-plagiarism-detection\n# Plagiarism Checker üîç\n\nA full-stack web application (Flask + React) to check plagiarism in uploaded text or Word documents using Natural Language Processing (NLP).  \nIt uses **SerpAPI** to search the web and **spaCy** to calculate content similarity.\n\nüöß _This project is still in development._\n\n---\n\n## üîß Features\n\n- Upload `.txt` or `.docx` files for plagiarism analysis.\n- Extracts random sentences from the document.\n- Google search integration via SerpAPI to find similar content.\n- NLP-powered similarity score calculation using **spaCy**.\n- Displays individual sentence matches with links and scores.\n- Overall plagiarism percentage.\n- **Frontend built with React** for a modern UI.\n\n---\n\n## üß† How It Works\n\n1. The file is uploaded through the React interface.\n2. The Flask backend reads and parses the document.\n3. Five random sentences are selected.\n4. Each sentence is searched on Google using SerpAPI (top 3 results).\n5. Content from those pages is fetched and compared using `spaCy`'s similarity method.\n6. Results are displayed in the frontend with links and match scores.\n7. Final plagiarism percentage is calculated from all matches.\n\n---\n\n## üóÇ Folder Structure\n\n```\nPlagiarism-Checker/\n‚îÇ\n‚îú‚îÄ‚îÄ frontend/             # React frontend\n‚îÇ   ‚îú‚îÄ‚îÄ public/\n‚îÇ   ‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îî‚îÄ‚îÄ ...\n‚îÇ\n‚îú‚îÄ‚îÄ templates/            # Flask HTML templates (for legacy UI)\n‚îú‚îÄ‚îÄ static/               # Static files (CSS, JS, images)\n‚îú‚îÄ‚îÄ nlpmain.py            # Flask backend using NLP\n‚îú‚îÄ‚îÄ main.py               # Alternate version (regex-based)\n‚îú‚îÄ‚îÄ requirements.txt      # Python dependencies\n‚îú‚îÄ‚îÄ .gitignore\n‚îú‚îÄ‚îÄ .env                  # API key (not committed)\n‚îî‚îÄ‚îÄ README.md\n```\n\n---\n\n## üöÄ Getting Started\n\n### üì¶ Prerequisites\n\n- Python 3.7+\n- Node.js + npm (for React frontend)\n- pip (Python package manager)\n- [SerpAPI](https://serpapi.com/) API key\n\n---\n\n### ‚öôÔ∏è Backend Setup (Flask)\n\n1. **Clone the repository**\n\n```bash\ngit clone https://github.com/LEADisDEAD/Plagiarism-Checker.git\ncd Plagiarism-Checker\n```\n\n2. **Create virtual environment**\n\n```bash\npython -m venv .venv\n.venv\\Scripts\\activate       # Windows\n# OR\nsource .venv/bin/activate   # macOS/Linux\n```\n\n3. **Install Python dependencies**\n\n```bash\npip install -r requirements.txt\n```\n\n4. **Add your SerpAPI key**\n\nCreate a `.env` file in the root folder:\n\n```env\nAPI_KEY=your_serpapi_key_here\n```\n\n_(Do not share this key publicly or push it to GitHub.)_\n\n5. **Download spaCy model**\n\n```bash\npython -m spacy download en_core_web_md\n```\n\n---\n\n### üé® Frontend Setup (React)\n\n1. **Navigate to frontend folder**\n\n```bash\ncd frontend\n```\n\n2. **Install dependencies**\n\n```bash\nnpm install\n```\n\n3. **Start the React app**\n\n```bash\nnpm start\n```\n\n4. **Frontend will run at** `http://localhost:3000` and connect to the Flask backend.\n\n---\n\n## üìù Notes\n\n- Make sure the Flask server is running on port 5000 and CORS is enabled.\n- You may need to adjust the proxy in `frontend/package.json` to match your Flask backend:\n```json\n\"proxy\": \"http://localhost:5000\"\n```\n\n- `.env` file is required both locally and in deployment (on platforms like Render or Vercel).\n- Do **not commit your `.env` file** ‚Äî it's already ignored via `.gitignore`.\n\n---\n\n## ‚ö†Ô∏è Alternate Backend Option\n\nThe `main.py` file has the same core logic but uses basic regex matching instead of NLP (spaCy).  \nIf you want a lighter and faster version with less accuracy, use this.\n\n---\n\n## ü§ù Contributing\n\nPull requests, issues, and feature suggestions are welcome!  \nFeel free to fork the repo and contribute.\n\n\n#  üßæAbstract\nPlagiarism has become a major issue in the academic and digital world, especially with the increasing availability of online documents, articles, and media. Today, individuals can easily copy text and images from the internet, modify them, and present them as original work, leading to copyright violations and ethical concerns. Traditional plagiarism detection systems mainly focus on text-only similarity and fail to detect plagiarism involving visual content such as images. Therefore, there is a need for a modern plagiarism detection system capable of analyzing both text and images.\nThis project proposes a combined text and image plagiarism detection system designed to identify copied or similar content across documents and digital images. For text plagiarism detection, methods such as Natural Language Processing, tokenization, semantic similarity, and cosine similarity are used to detect exact or paraphrased plagiarism even after changing sentence structure or wording. These techniques allow deeper understanding and comparison of meaning rather than depending on direct text matching.\nFor image plagiarism detection, the system applies computer vision and deep learning techniques to identify similarities between images. Feature extraction models such as ORB, CNN and SSIM enable detection of altered, rotated, resized, filtered, or partially modified images. By matching visual patterns and pixel structures, the system accurately identifies whether an uploaded image is plagiarized or unique.\nThe overall goal of this project is to provide a reliable platform that can automatically detect plagiarism in textual and visual content, generate plagiarism percentage reports, and highlight matched portions. This system helps students, researchers, faculty, and content creators to maintain originality and prevent copyright violations. The combined text and image approach increases accuracy and overcomes the limitations of traditional plagiarism detectors, making the system modern, robust, and suitable for real-world applications.\n\n\n#  ‚öôÔ∏è 2. Working / Methodology\nThe plagiarism detection system works in a step-by-step pipeline:\n\n1)User Input: The user uploads a .txt or .docx document through the React-based frontend.\n\n2)File Processing: The Flask backend parses the document and extracts clean textual content.\n\n3)Sentence Selection: Five random sentences are selected to represent the document.\n\n4)Web Search: Each sentence is queried on Google via SerpAPI, retrieving the top three matching web pages.\n\n5)Content Extraction: Textual content from the retrieved URLs is scraped.\n\n6)NLP Similarity Analysis: spaCy‚Äôs en_core_web_md model calculates semantic similarity between the uploaded sentence and web content.\n\n7)Score Calculation: Individual similarity scores are aggregated to compute an overall plagiarism percentage.\n\n8)Result Display: The frontend displays sentence-wise matches, similarity scores, source links, and final plagiarism percentage.\n\n\n#  üìê 3. UML Diagrams (9 Types)\n\n1)üìä Activity UML Diagram\n\n![img alt](https://github.com/Universal-college-projects/Image-and-text-plagiarism-detection/blob/7203026b00b36250e8cac39e382ea4702680708f/Activity-uml-diagram.png.png)\n\n2)üß± Component UML Diagram\n\n![img alt](https://github.com/Universal-college-projects/Image-and-text-plagiarism-detection/blob/b483f1b5e9f8809c820a47e1eada9e5f2bbcda1a/Component-uml-diagram.png.png)\n\n3)üß© Object UML Diagram\n\n![img alt](https://github.com/Universal-college-projects/Image-and-text-plagiarism-detection/blob/b483f1b5e9f8809c820a47e1eada9e5f2bbcda1a/Object-uml-diagram.png.png)\n\n4)üîÅ Sequence UML Diagram\n\n![img alt](https://github.com/Universal-college-projects/Image-and-text-plagiarism-detection/blob/b483f1b5e9f8809c820a47e1eada9e5f2bbcda1a/Sequence-uml-diagram.png.png)\n\n5)üß© Class UML Diagram\n\n![img alt](https://github.com/Universal-college-projects/Image-and-text-plagiarism-detection/blob/b483f1b5e9f8809c820a47e1eada9e5f2bbcda1a/class-uml-diagram.png.png)\n\n6)üì° Communication UML Diagram\n\n![img alt](https://github.com/Universal-college-projects/Image-and-text-plagiarism-detection/blob/b483f1b5e9f8809c820a47e1eada9e5f2bbcda1a/communication-uml-diagram.png.png)\n\n7)üì¶ Package UML Diagram\n\n![img alt](https://github.com/Universal-college-projects/Image-and-text-plagiarism-detection/blob/b483f1b5e9f8809c820a47e1eada9e5f2bbcda1a/package-uml-diagram.png.png)\n\n8)üîÑ State UML Diagram (State Machine Diagram)\n\n![img alt](https://github.com/Universal-college-projects/Image-and-text-plagiarism-detection/blob/b483f1b5e9f8809c820a47e1eada9e5f2bbcda1a/state-uml-diagra%20m.png.png)\n\n9)üë§ Use Case UML Diagram\n\n![img alt](https://github.com/Universal-college-projects/Image-and-text-plagiarism-detection/blob/b483f1b5e9f8809c820a47e1eada9e5f2bbcda1a/use-case%20-uml-diagram.png.png)\n\n\n4)üß† Training Images / Data\n\n\n‚úî Training Data / Training Images Section \n\nTraining Data:\nThe system uses a pre-trained Natural Language Processing model (spaCy ‚Äì en_core_web_md).\nNo custom training images or datasets were required. The model was trained on large-scale English text corpora including news articles, web text, and Wikipedia.\n\nüìå There are NO training images, because:\n\nThis is text plagiarism, not image ML training\n\nNo CNN / deep learning model is used\n\n\n5)üîπ Result Images\n\nüñºÔ∏è 1. Upload Interface\n\nReact UI where user uploads .txt or .docx /.png file\nüìå Filename:project_Interface.png\n\n![img alt](https://github.com/Universal-college-projects/Image-and-text-plagiarism-detection/blob/b0ed12c9837b1beaab26fa1d8d7d9dec27a4d5b3/Program_Interface_git.png..jpeg)\n\n                                  \nüñºÔ∏è 2. Processing Screen\n\nLoading state while plagiarism is being checked\n\nüìå Filename : Upload_Interface.png\n\n  ![img alt](https://github.com/Universal-college-projects/Image-and-text-plagiarism-detection/blob/f45fb68bc2f6cd37fedd6df850e47383a4fa88bf/Analyzing.png.jpeg)\n\nüñºÔ∏è 3. Sentence-wise Plagiarism Result\n\nShows:\n\n->Sentence\n\n->Similarity score \n\nüìå Filename :Sentence_match.png\n\n![img alt](https://github.com/Universal-college-projects/Image-and-text-plagiarism-detection/blob/6cc3e7b364b71d40bc3b6f148640d5ff6af02178/sentece_check.jpg.jpeg)\n\n\n\nüñºÔ∏è 4.Final_Result\n\nüìå Filename :Final_Result.png\n\n![img alt](https://github.com/Universal-college-projects/Image-and-text-plagiarism-detection/blob/6cc3e7b364b71d40bc3b6f148640d5ff6af02178/image_checker2.png.jpeg)\n\n\n\n# üõ†Ô∏è Software Tools Used\n\nThe development of the **Image and Text Plagiarism Detection System** involves a combination of backend, frontend, and support tools to ensure a smooth and scalable workflow. Below is a detailed description of each tool:\n\n1. **Python 3.7+**\n\n   * **Purpose:** Backend development, text processing, and NLP operations.\n   * **Why Used:** Python provides excellent libraries for natural language processing, web requests, and file handling, making it ideal for implementing plagiarism detection logic.\n\n2. **Flask**\n\n   * **Purpose:** Web framework for the backend.\n   * **Why Used:** Lightweight, flexible, and easy to integrate with Python NLP libraries. Handles REST API creation for connecting the frontend and backend.\n\n3. **React.js**\n\n   * **Purpose:** Frontend user interface.\n   * **Why Used:** Provides a dynamic, responsive, and modern web interface for users to upload files, view plagiarism results, and interact with the system.\n\n4. **Node.js & npm**\n\n   * **Purpose:** Frontend environment and dependency management.\n   * **Why Used:** Required to run React development server and install frontend libraries.\n\n5. **spaCy**\n\n   * **Purpose:** Natural Language Processing (NLP) library.\n   * **Why Used:** Calculates semantic similarity between sentences to detect plagiarism at the sentence level using pre-trained models (`en_core_web_md`).\n\n6. **SerpAPI**\n\n   * **Purpose:** Google Search API integration.\n   * **Why Used:** To fetch top web results for sentences from uploaded documents for comparison, enabling real-time plagiarism detection.\n\n7. **VS Code / IDE**\n\n   * **Purpose:** Development environment.\n   * **Why Used:** Provides code editing, debugging, and project organization tools.\n\n8. **Git & GitHub**\n\n   * **Purpose:** Version control and repository hosting.\n   * **Why Used:** Tracks changes, supports collaboration, and hosts project code and documentation online.\n\n9. **Postman (Optional)**\n\n   * **Purpose:** API testing.\n   * **Why Used:** Verifies backend API endpoints independently from the frontend.\n\n---\n\n# üìö Code Libraries Used\n\nThe project uses **Python libraries** for backend processing and **JavaScript libraries** for frontend UI operations. Each library contributes to specific functionality in the plagiarism detection pipeline.\n\n### **Python Libraries (Backend)**\n\n| Library           | Purpose                                                                                           |\n| ----------------- | ------------------------------------------------------------------------------------------------- |\n| **flask**         | Creates the web server and routes for API endpoints.                                              |\n| **flask-cors**    | Enables Cross-Origin Resource Sharing to allow frontend-backend communication.                    |\n| **spacy**         | Performs NLP operations, including tokenization and semantic similarity calculations.             |\n| **python-docx**   | Reads and extracts text from Word documents (`.docx`).                                            |\n| **requests**      | Sends HTTP requests to SerpAPI and fetches web content.                                           |\n| **python-dotenv** | Loads environment variables such as API keys from a `.env` file.                                  |\n| **random**        | Selects random sentences from documents for plagiarism checks.                                    |\n| **re**            | Provides regex support for text cleaning and optional regex-based matching (alternative backend). |\n\n### **JavaScript / React Libraries (Frontend)**\n\n| Library              | Purpose                                               |\n| -------------------- | ----------------------------------------------------- |\n| **react**            | Provides component-based frontend structure.          |\n| **axios**            | Handles HTTP requests to Flask backend API endpoints. |\n| **react-router-dom** | Enables routing for multi-page React applications.    |\n| **bootstrap / CSS**  | Provides styling and responsive design components.    |\n\n\n\n## üë©‚Äçüíª Authors\n\n**Prathmesh Manoj Bajpai**  \n[LinkedIn](https://www.linkedin.com/in/prathmesh-bajpai-8429652aa/)\n\n**Aditi Ritesh Dixit**  \n[LinkedIn](https://www.linkedin.com/in/aditi-dixit-895b551b5/)\n","setup_steps":[],"technologies":["Fortran","JavaScript","Jupyter Notebook","Roff","Jinja","C","Meson","XSLT","Shell","React"],"stars":0,"forks":0,"last_updated":"2025-12-22T05:23:52Z","created_at":"2025-12-09T10:18:37Z","default_branch":"main"},{"name":"cyber-threats","department":"AI","title":"Cyber Threats","description":"No description available","repo_url":"https://github.com/Universal-college-projects/cyber-threats","demo_url":null,"languages":["Python","CSS","JavaScript","HTML"],"topics":[],"difficulty":"intermediate","estimated_time":"Varies","readme_content":"<<<<<<< HEAD\r\n# cyber-threats\r\nhttps://github.com/AyyubAnsari786/Cyber-Threat-Detection-Based-On-Artificial-Neural-Networks-Using-Event-Profiles\r\nhttps://github.com/vakalatha/CYBER_THREAT_DETECTION_BASED_ON_ARTIFICIAL_NEURAL_NETWORKS_USING_EVENT_PROFILES_MACHINE_LEARNING\r\n=======\r\n# AI-SIEM Cyber Threat Detection System\r\n\r\n## üéØ Aim of the Project\r\nThe primary goal of this project is to automate the detection of cyber threats (such as network intrusions and malicious attacks) using **Artificial Intelligence**.\r\n\r\nTraditional security systems often flood analysts with false alarms. This project uses **Deep Learning** to analyze patterns in network logs, helping to identify real threats more accurately and efficiently.\r\n\r\nIt implements the concepts from the research paper *\"Cyber Threat Detection Based on Artificial Neural Networks Using Event Profiles\"*, utilizing three distinct AI models:\r\n- **FCNN**: For general pattern recognition.\r\n- **CNN**: For detecting local anomalies in event data.\r\n- **LSTM**: For identifying malicious sequences over time.\r\n\r\n---\r\n\r\n## üöÄ How to Use It\r\n\r\n### Prerequisites\r\nEnsure you have **Python 3.9+** installed on your system.\r\n\r\n### 1. Open in VS Code\r\n1. Open Visual Studio Code.\r\n2. Go to **File > Open Folder** and select the `CS project` folder.\r\n3. Open a **Terminal** inside VS Code (`Ctrl + ` or `Terminal > New Terminal`).\r\n\r\n### 2. Install Dependencies (First Time Only)\r\nIn the VS Code terminal, run:\r\n```powershell\r\npip install -r requirements.txt\r\n```\r\n\r\n### 3. Train the AI Models (Important!)\r\nBefore the system can detect anything, it needs to \"learn\". Run this command to generate data and train the brains of the system:\r\n```powershell\r\npython backend/src/train.py\r\n```\r\n*Wait until you see \"Training complete\" and \"Models saved\". This creates the `.h5` model files in `backend/data`.*\r\n### 4. Start the Web Dashboard\r\nRun the main application server:\r\n```powershell\r\npython backend/src/app.py\r\n```\r\nYou should see a message: `Running on http://127.0.0.1:5000`.\r\n\r\n### 5. Detect Threats\r\n1. Open your web browser (Chrome/Edge).\r\n2. Go to: **[http://127.0.0.1:5000](http://127.0.0.1:5000)**\r\n3. **Upload Log File**: Drag and drop the `valid_test.csv` file (located in your project folder) into the upload box.\r\n4. Click **Run AI Analysis**.\r\n5. The AI will scan the file and display which IP addresses are \"THREATS\" and which are \"NORMAL\".\r\n\r\n---\r\n\r\n## üìÇ Project Structure\r\n- **`backend/src/train.py`**: The \"Teacher\". Generates synthetic data and trains the AI models.\r\n- **`backend/src/app.py`**: The \"Server\". Connects the web dashboard to the Python AI.\r\n- **`backend/src/preprocessing.py`**: The \"Translator\". Converts raw logs into math that the AI understands (TF-IDF, Sliding Windows).\r\n- **`backend/src/models.py`**: The \"Brain\". Contains the code for the FCNN, CNN, and LSTM neural networks.\r\n- **`frontend/`**: Contains the beautiful web interface (`index.html`, `style.css`, `script.js`).\r\n- **`valid_test.csv`**: A sample file provided for you to test the system immediately.\r\n\r\n---\r\n\r\n## ‚ùì Troubleshooting\r\n- **\"Models not found\"**: You likely forgot to run `python backend/src/train.py` first.\r\n- **\"JSON Error\"**: Make sure you are uploading a valid CSV file (like `valid_test.csv`) with the correct columns: `timestamp`, `source_ip`, `event_id`.\r\n>>>>>>> a16637f (FIRST)\r\n","setup_steps":[],"technologies":["CSS","JavaScript","Python","HTML"],"stars":0,"forks":0,"last_updated":"2025-12-21T15:23:49Z","created_at":"2025-12-09T09:53:43Z","default_branch":"main"},{"name":"solar-panel-detection-clasification","department":"AI","title":"Solar Panel Detection Clasification","description":"No description available","repo_url":"https://github.com/Universal-college-projects/solar-panel-detection-clasification","demo_url":null,"languages":["Python","HTML"],"topics":[],"difficulty":"advanced","estimated_time":"Varies","readme_content":"#Enhancing Defect classification in solar panels with Electroluminescence Imaging and Advanced Machine learning and deep learning and Ai by using YOLOV8 \n\n\nSolar Panel Defect Classification using Electroluminescence (EL) Imaging & YOLOv8\n\nProject Title: Enhancing Defect Classification in Solar Panels using Electroluminescence Imaging, Machine Learning, Deep Learning & YOLOv8\nOverview\n\nThis project focuses on detecting and classifying defects in photovoltaic (PV) solar panels using Electroluminescence (EL) images and YOLOv8 deep learning model. EL imaging highlights hidden defects like micro-cracks, hotspots, and broken cell fingers, making it an ideal method for automated inspection.\n\nThis repository includes dataset structure, training pipeline, model configuration, evaluation metrics, and deployment steps to build a complete AI-based defect inspection system.\n\nObjective\n\nDetect various solar panel defects using EL images.\n\nTrain an accurate, real-time YOLOv8-based detection model.\n\nDeploy the model for practical field inspections using edge devices or cloud.\n\nAutomate reporting for maintenance teams.\n\nDefects Covered\n\nRecommended classes:\n\nmicro_crack\n\nmajor_crack\n\nbroken_finger\n\ninactive_cell\n\nhotspot\n\ndelamination\n\nsolder_break\n\n(You may add/remove classes depending on your dataset.)\n\nDataset Structure\nproject-root/\n‚îÇ\n‚îú‚îÄ‚îÄ images/\n‚îÇ   ‚îú‚îÄ‚îÄ train/\n‚îÇ   ‚îú‚îÄ‚îÄ val/\n‚îÇ   ‚îî‚îÄ‚îÄ test/\n‚îÇ\n‚îú‚îÄ‚îÄ labels/\n‚îÇ   ‚îú‚îÄ‚îÄ train/\n‚îÇ   ‚îú‚îÄ‚îÄ val/\n‚îÇ   ‚îî‚îÄ‚îÄ test/\n‚îÇ\n‚îî‚îÄ‚îÄ data.yaml\nSample data.yaml\ntrain: ./images/train\nval: ./images/val\ntest: ./images/test\n\n\nnc: 7\nnames: [micro_crack, major_crack, broken_finger, inactive_cell, hotspot, delamination, solder_break]\nAnnotation Format (YOLO Format)\n\nEach annotation file (.txt) contains:\n\nclass_id  center_x  center_y  width  height\n\nAll values are normalized (0‚Äì1).\n\nTools you can use:\n\nLabelImg\n\nRoboflow Annotate\n\nCVAT\n\nLabel Studio\n\nInstallation\npip install ultralytics opencv-python numpy\n\nOr clone YOLOv8 directly:\n\npip install ultralytics\nTraining YOLOv8\nTrain using CLI:\nyolo task=detect mode=train model=yolov8s.pt data=data.yaml imgsz=640 epochs=50 batch=16\nTrain using Python:\nfrom ultralytics import YOLO\n\n\nmodel = YOLO('yolov8s.pt')\nmodel.train(data='data.yaml', epochs=50, imgsz=640)\nEvaluation\n\nRun validation:\n\nyolo mode=val model=runs/detect/train/weights/best.pt data=data.yaml\n\nMetrics produced:\n\nmAP@0.5\n\nmAP@0.5:0.95\n\nPrecision & Recall\n\nConfusion Matrix\n\nPer-class AP\n\nInference\nOn an image:\nyolo predict model=best.pt source=path/to/image.jpg\nWith Python:\nmodel = YOLO('best.pt')\nresults = model('image.jpg')\nresults.show()\nModel Export\nyolo export model=best.pt format=onnx\n\nSupported formats:\n\nTensorRT\n\nONNX\n\nCoreML\n\nOpenVINO\n\nTFLite\n\nDeployment Options\nEdge Deployment:\n\nNVIDIA Jetson Nano / Xavier (TensorRT)\n\nRaspberry Pi + Coral TPU\n\nIntel NCS2 (OpenVINO)\n\nCloud Deployment:\n\nFlask / FastAPI REST API\n\nStreamlit dashboard\n\nMobile/Web inspection tool\n\nAugmentations Used\n\nHorizontal/Vertical flip\n\nBrightness/contrast shift\n\nGaussian noise\n\nRandom rotation\n\nMosaic (YOLO built-in)\n\nCutMix (optional)\n\n# A Benchmark for Visual Identification of Defective Solar Cells in Electroluminescence Imagery\n\n[![PyPI - Version](https://img.shields.io/pypi/v/elpv-dataset.svg)](https://pypi.org/project/elpv-dataset)\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/elpv-dataset.svg)](https://pypi.org/project/elpv-dataset)\n\nThis repository provides a dataset of solar cell images extracted from\nhigh-resolution electroluminescence images of photovoltaic modules.\n\n![An overview of images in the dataset. The darker the red is, the higher is the\nlikelihood of a defect in the solar cell overlayed by the corresponding color.](./doc/images/overview.jpg)\n\n## The Dataset\n\nThe dataset contains 2,624 samples of 300x300 pixels 8-bit grayscale images of\nfunctional and defective solar cells with varying degree of degradations\nextracted from 44 different solar modules. The defects in the annotated images\nare either of intrinsic or extrinsic type and are known to reduce the power\nefficiency of solar modules.\n\nAll images are normalized with respect to size and perspective.\nAdditionally, any distortion induced by the camera lens used to capture the EL images was\neliminated prior to solar cell extraction.\n\n## Annotations\n\nEvery image is annotated with a defect probability (a floating point value\nbetween 0 and 1) and the type of the solar module (either mono- or\npolycrystalline) the solar cell image was originally extracted from.\n\n## Usage\n\nInstall the Python package\n```console\npip install elpv-dataset\n```\n\nand load the images and the corresponding annotations as follows:\n\n```python\nfrom elpv_dataset.utils import load_dataset\nimages, proba, types = load_dataset()\n```\n\n## Citing\n\nIf you use this dataset in scientific context, please cite the following\npublications:\n\n> Buerhop-Lutz, C.; Deitsch, S.; Maier, A.; Gallwitz, F.; Berger, S.; Doll, B.; Hauch, J.; Camus, C. & Brabec, C. J. A Benchmark for Visual Identification of Defective Solar Cells in Electroluminescence Imagery. European PV Solar Energy Conference and Exhibition (EU PVSEC), 2018. DOI: [10.4229/35thEUPVSEC20182018-5CV.3.15](http://dx.doi.org/10.4229/35thEUPVSEC20182018-5CV.3.15)\n\n> Deitsch, S., Buerhop-Lutz, C., Sovetkin, E., Steland, A., Maier, A., Gallwitz, F., & Riess, C. (2021). Segmentation of photovoltaic module cells in uncalibrated electroluminescence images. Machine Vision and Applications, 32(4). DOI: [10.1007/s00138-021-01191-9](https://doi.org/10.1007/s00138-021-01191-9)\n\n> Deitsch, S.; Christlein, V.; Berger, S.; Buerhop-Lutz, C.; Maier, A.; Gallwitz, F. & Riess, C. Automatic classification of defective photovoltaic module cells in electroluminescence images. Solar Energy, Elsevier BV, 2019, 185, 455-468. DOI: [10.1016/j.solener.2019.02.067](http://dx.doi.org/10.1016/j.solener.2019.02.067)\n\nBibTeX details:\n\n<details>\n\n```bibtex\n\n@InProceedings{Buerhop2018,\n  author    = {Buerhop-Lutz, Claudia and Deitsch, Sergiu and Maier, Andreas and Gallwitz, Florian and Berger, Stephan and Doll, Bernd and Hauch, Jens and Camus, Christian and Brabec, Christoph J.},\n  title     = {A Benchmark for Visual Identification of Defective Solar Cells in Electroluminescence Imagery},\n  booktitle = {European PV Solar Energy Conference and Exhibition (EU PVSEC)},\n  year      = {2018},\n  eventdate = {2018-09-24/2018-09-28},\n  venue     = {Brussels, Belgium},\n  doi       = {10.4229/35thEUPVSEC20182018-5CV.3.15},\n}\n\n@Article{Deitsch2021,\n  author       = {Deitsch, Sergiu and Buerhop-Lutz, Claudia and Sovetkin, Evgenii and Steland, Ansgar and Maier, Andreas and Gallwitz, Florian and Riess, Christian},\n  date         = {2021},\n  journaltitle = {Machine Vision and Applications},\n  title        = {Segmentation of photovoltaic module cells in uncalibrated electroluminescence images},\n  doi          = {10.1007/s00138-021-01191-9},\n  issn         = {1432-1769},\n  number       = {4},\n  volume       = {32},\n}\n\n@Article{Deitsch2019,\n  author    = {Sergiu Deitsch and Vincent Christlein and Stephan Berger and Claudia Buerhop-Lutz and Andreas Maier and Florian Gallwitz and Christian Riess},\n  title     = {Automatic classification of defective photovoltaic module cells in electroluminescence images},\n  journal   = {Solar Energy},\n  year      = {2019},\n  volume    = {185},\n  pages     = {455--468},\n  month     = jun,\n  issn      = {0038-092X},\n  doi       = {10.1016/j.solener.2019.02.067},\n  publisher = {Elsevier {BV}},\n}\n```\n</details>\n\n## License\n\n<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a><br />All the images in this work are licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>. Accompanying Python source code is distributed under the terms of the [Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0.html).\n\n\nFor commercial use, please contact us for further information.\n\n\n\n preprocess\n\nimport os\nimport shutil\nimport pandas as pd\nimport numpy as np\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom glob import glob\n\n# Configuration\nDATASET_DIR = r'C:\\Users\\devas\\solar project\\data\\elpv-dataset'\nOUTPUT_DIR = r'C:\\Users\\devas\\solar project\\data\\processed'\nIMG_SIZE = (640, 640)\nRANDOM_SEED = 42\n\ndef find_labels_file(start_dir):\n    for root, dirs, files in os.walk(start_dir):\n        if 'labels.csv' in files:\n            return os.path.join(root, 'labels.csv')\n    return None\n\ndef preprocess():\n    print(\"Finding labels.csv...\")\n    labels_path = find_labels_file(DATASET_DIR)\n    if not labels_path:\n        print(f\"Error: labels.csv not found in {DATASET_DIR}. Did git clone finish?\")\n        return\n\n    print(f\"Found labels at: {labels_path}\")\n    df = pd.read_csv(labels_path, sep='\\s+|,', engine='python') # Handle potential space or comma separation\n    \n    # Check columns\n    # ELPV usually has no header, or specific columns. Let's inspect first row if needed.\n    # Standard ELPV: image_path, defect_probability, type\n    # But usually it's space separated: \"images/cell0001.png 0.0 mono\"\n    \n    # If no header, we assume:\n    if len(df.columns) < 2:\n         df = pd.read_csv(labels_path, delim_whitespace=True, header=None, names=['image_name', 'prob', 'type'])\n\n    # Standardize\n    # Make sure we have image_name and prob\n    if 'prob' not in df.columns:\n        # try 2nd column\n        df.columns = ['image_name', 'prob', 'type']\n    \n    print(f\"Loaded {len(df)} samples.\")\n\n    # Define classes: 0.0 = Normal, > 0.0 (or >0.5) = Defected?\n    # Paper says: 0=functional, 1=defective. Intermediate = probability.\n    # User step 2 says: \"Normal / Defected\".\n    # We will threshold at 0.5.\n    df['label'] = df['prob'].apply(lambda x: 'defected' if x >= 0.5 else 'normal')\n    \n    print(df['label'].value_counts())\n\n    # Split\n    # Stratified split to keep balance\n    train_df, test_df = train_test_split(df, test_size=0.1, stratify=df['label'], random_state=RANDOM_SEED)\n    train_df, val_df = train_test_split(train_df, test_size=0.22, stratify=train_df['label'], random_state=RANDOM_SEED) # 0.22 of 0.9 ~ 0.2 total\n\n    print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n\n    # clear output dir\n    if os.path.exists(OUTPUT_DIR):\n        shutil.rmtree(OUTPUT_DIR)\n    \n    for subdir in ['train', 'val', 'test']:\n        for label in ['normal', 'defected']:\n            os.makedirs(os.path.join(OUTPUT_DIR, subdir, label), exist_ok=True)\n\n    # Processing function\n    def process_subset(subset_df, subset_name):\n        dataset_root = os.path.dirname(labels_path)\n        for idx, row in subset_df.iterrows():\n            img_path = os.path.join(dataset_root, row['image_name'])\n            \n            # Handle if image_name contains \"images/\" prefix or not\n            if not os.path.exists(img_path):\n                # try removing/adding images prefix\n                if row['image_name'].startswith('images/'):\n                     img_path = os.path.join(dataset_root, row['image_name'])\n                else:\n                     img_path = os.path.join(dataset_root, 'images', row['image_name'])\n            \n            if not os.path.exists(img_path):\n                print(f\"Warning: Image not found {img_path}\")\n                continue\n                \n            # Load and resize\n            img = cv2.imread(img_path)\n            if img is None:\n                print(f\"Warning: Failed to load {img_path}\")\n                continue\n                \n            img_resized = cv2.resize(img, IMG_SIZE)\n            \n            # Save\n            save_name = f\"{subset_name}_{os.path.basename(row['image_name'])}\"\n            save_path = os.path.join(OUTPUT_DIR, subset_name, row['label'], save_name)\n            cv2.imwrite(save_path, img_resized)\n            \n    print(\"Processing Train...\")\n    process_subset(train_df, 'train')\n    print(\"Processing Val...\")\n    process_subset(val_df, 'val')\n    print(\"Processing Test...\")\n    process_subset(test_df, 'test')\n    \n    print(\"Preprocessing complete!\")\n\nif __name__ == '__main__':\n    preprocess()\n\n\n    \n\nscript.js\n\ndocument.addEventListener('DOMContentLoaded', () => {\n    const dropZone = document.getElementById('drop-zone');\n    const fileInput = document.getElementById('file-input');\n    const previewContainer = document.getElementById('preview-container');\n    const imagePreview = document.getElementById('image-preview');\n    const removeBtn = document.getElementById('remove-file');\n    const analyzeBtn = document.getElementById('analyze-btn');\n    const uploadForm = document.getElementById('upload-form');\n    const spinner = document.getElementById('spinner');\n    \n    // Result elements\n    const resultSection = document.getElementById('result-section');\n    const resultImage = document.getElementById('result-image');\n    const statusBadge = document.getElementById('status-badge');\n    const meterFill = document.getElementById('meter-fill');\n    const confidenceValue = document.getElementById('confidence-value');\n    const maintenanceMsg = document.getElementById('maintenance-msg');\n    \n    // Drag & Drop\n    ['dragenter', 'dragover', 'dragleave', 'drop'].forEach(eventName => {\n        dropZone.addEventListener(eventName, preventDefaults, false);\n    });\n    \n    function preventDefaults(e) {\n        e.preventDefault();\n        e.stopPropagation();\n    }\n    \n    ['dragenter', 'dragover'].forEach(eventName => {\n        dropZone.addEventListener(eventName, () => dropZone.classList.add('dragover'), false);\n    });\n    \n    ['dragleave', 'drop'].forEach(eventName => {\n        dropZone.addEventListener(eventName, () => dropZone.classList.remove('dragover'), false);\n    });\n    \n    dropZone.addEventListener('drop', handleDrop, false);\n    \n    function handleDrop(e) {\n        const dt = e.dataTransfer;\n        const files = dt.files;\n        handleFiles(files);\n    }\n    \n    // Click upload\n    dropZone.addEventListener('click', () => fileInput.click());\n    fileInput.addEventListener('change', function() {\n        handleFiles(this.files);\n    });\n    \n    function handleFiles(files) {\n        if (files.length > 0) {\n            const file = files[0];\n            if (file.type.startsWith('image/')) {\n                const reader = new FileReader();\n                reader.onload = (e) => {\n                    imagePreview.src = e.target.result;\n                    previewContainer.classList.remove('hidden');\n                    dropZone.classList.add('hidden');\n                    analyzeBtn.disabled = false;\n                };\n                reader.readAsDataURL(file);\n                \n                // Manually set files to input if dropped (a bit hacky but works for form submit)\n                // Actually easier to just append to FormData later\n            }\n        }\n    }\n    \n    removeBtn.addEventListener('click', () => {\n        fileInput.value = '';\n        previewContainer.classList.add('hidden');\n        dropZone.classList.remove('hidden');\n        analyzeBtn.disabled = true;\n        resultSection.classList.add('hidden');\n    });\n    \n    // Submit\n    uploadForm.addEventListener('submit', async (e) => {\n        e.preventDefault();\n        \n        const files = fileInput.files;\n        // If dropped, input might be empty, need to handle that if needed, \n        // but for now we assume input was populated or clicked.\n        // If fileInput is empty, maybe drag&drop didn't populate it.\n        // Let's rely on standard input selection for simplicity in this code block, \n        // or check if dropped file exists.\n        \n        if (files.length === 0) return;\n        \n        const formData = new FormData();\n        formData.append('file', files[0]);\n        \n        // UI Loading\n        analyzeBtn.disabled = true;\n        spinner.classList.remove('hidden');\n        analyzeBtn.querySelector('span').textContent = 'Analyzing...';\n        resultSection.classList.add('hidden');\n        \n        try {\n            const response = await fetch('/predict', {\n                method: 'POST',\n                body: formData\n            });\n            \n            const data = await response.json();\n            \n            if (response.ok) {\n                showResult(data);\n            } else {\n                alert('Analysis failed: ' + (data.error || 'Unknown error'));\n            }\n            \n        } catch (error) {\n            console.error(error);\n            alert('An error occurred during analysis.');\n        } finally {\n            analyzeBtn.disabled = false;\n            spinner.classList.add('hidden');\n            analyzeBtn.querySelector('span').textContent = 'Analyze Panel';\n        }\n    });\n    \n    function showResult(data) {\n        resultSection.classList.remove('hidden');\n        resultImage.src = data.image_url;\n        \n        // Data: class (normal/defected), confidence (0.0-1.0)\n        let cls = data.class.toLowerCase();\n        const confidence = parseFloat(data.confidence);\n        const confidencePct = Math.round(confidence * 100);\n        \n        // Update badge\n        statusBadge.className = 'status-badge'; \n        if (cls.includes('defect') || cls.includes('bad')) {\n            statusBadge.classList.add('defected');\n            statusBadge.textContent = 'DEFECT DETECTED';\n            maintenanceMsg.textContent = 'Warning: Structural anomalies detected. Immediate maintenance or further inspection is recommended to prevent efficiency loss.';\n            maintenanceMsg.style.color = '#ff3344';\n        } else if (cls.includes('normal') || cls.includes('good')) {\n            statusBadge.classList.add('normal');\n            statusBadge.textContent = 'PANEL HEALTHY';\n            maintenanceMsg.textContent = 'No significant defects detected. The solar panel appears to be in optimal operating condition.';\n            maintenanceMsg.style.color = '#00ff88';\n        } else {\n            // Fallback for unknown classes (e.g. from pretrained model)\n            statusBadge.textContent = cls.toUpperCase();\n            maintenanceMsg.textContent = `Identified as ${cls}.`;\n            maintenanceMsg.style.color = '#fff';\n        }\n        \n        // Update Meter\n        meterFill.style.width = `${confidencePct}%`;\n        confidenceValue.textContent = `${confidencePct}% Confidence`;\n        \n        // Scroll to result\n        resultSection.scrollIntoView({ behavior: 'smooth' });\n    }\n});\n\n\n\n\n\n\nstyle.css\n\n:root {\n    --bg-color: #050a14;\n    --text-color: #ffffff;\n    --accent-color: #ffaa00; /* Solar Gold */\n    --accent-glow: rgba(255, 170, 0, 0.4);\n    --glass-bg: rgba(255, 255, 255, 0.05);\n    --glass-border: rgba(255, 255, 255, 0.1);\n    --success-color: #00ff88;\n    --danger-color: #ff3344;\n}\n\n* {\n    margin: 0;\n    padding: 0;\n    box-sizing: border-box;\n    font-family: 'Outfit', sans-serif;\n}\n\nbody {\n    background-color: var(--bg-color);\n    color: var(--text-color);\n    min-height: 100vh;\n    overflow-x: hidden;\n    display: flex;\n    justify-content: center;\n    align-items: center;\n    position: relative;\n}\n\n/* Background Orbs */\n.background-orb {\n    position: absolute;\n    border-radius: 50%;\n    filter: blur(80px);\n    z-index: -1;\n    opacity: 0.6;\n}\n.orb-1 {\n    top: -10%;\n    left: -10%;\n    width: 600px;\n    height: 600px;\n    background: radial-gradient(circle, #1a237e, transparent);\n}\n.orb-2 {\n    bottom: -10%;\n    right: -10%;\n    width: 500px;\n    height: 500px;\n    background: radial-gradient(circle, #311b92, transparent);\n}\n\n.container {\n    width: 90%;\n    max-width: 1000px;\n    padding: 2rem;\n    z-index: 1;\n}\n\nheader {\n    text-align: center;\n    margin-bottom: 3rem;\n    animation: fadeInDown 0.8s ease-out;\n}\n\n.logo h1 {\n    font-size: 3rem;\n    font-weight: 700;\n    background: linear-gradient(to right, #fff, var(--accent-color));\n    -webkit-background-clip: text;\n    -webkit-text-fill-color: transparent;\n    display: inline-block;\n}\n\n.logo .icon {\n    font-size: 2.5rem;\n    vertical-align: middle;\n    margin-right: 10px;\n}\n\n.subtitle {\n    color: rgba(255, 255, 255, 0.7);\n    font-weight: 300;\n    letter-spacing: 1px;\n}\n\n.glass-card {\n    background: var(--glass-bg);\n    backdrop-filter: blur(16px);\n    border: 1px solid var(--glass-border);\n    border-radius: 20px;\n    padding: 2rem;\n    box-shadow: 0 8px 32px 0 rgba(0, 0, 0, 0.37);\n    transition: transform 0.3s ease;\n}\n\n.upload-section {\n    text-align: center;\n    animation: fadeInUp 0.8s ease-out 0.2s backwards;\n}\n\n.drop-zone {\n    border: 2px dashed rgba(255, 255, 255, 0.2);\n    border-radius: 12px;\n    padding: 3rem;\n    margin: 2rem 0;\n    cursor: pointer;\n    transition: all 0.3s;\n}\n\n.drop-zone:hover, .drop-zone.dragover {\n    border-color: var(--accent-color);\n    background: rgba(255, 170, 0, 0.05);\n}\n\n.drop-icon {\n    font-size: 3rem;\n    display: block;\n    margin-bottom: 1rem;\n}\n\n.btn-primary {\n    background: linear-gradient(135deg, var(--accent-color), #ff8800);\n    border: none;\n    padding: 1rem 3rem;\n    border-radius: 50px;\n    color: #fff;\n    font-size: 1.1rem;\n    font-weight: 600;\n    cursor: pointer;\n    transition: all 0.3s;\n    box-shadow: 0 4px 15px var(--accent-glow);\n    display: flex;\n    align-items: center;\n    justify-content: center;\n    margin: 0 auto;\n    min-width: 200px;\n}\n\n.btn-primary:hover:not(:disabled) {\n    transform: translateY(-2px);\n    box-shadow: 0 6px 20px var(--accent-glow);\n}\n\n.btn-primary:disabled {\n    opacity: 0.5;\n    cursor: not-allowed;\n    transform: none;\n}\n\n/* Preview */\n.preview-container {\n    position: relative;\n    max-width: 300px;\n    margin: 2rem auto;\n}\n\n.preview-container img {\n    width: 100%;\n    border-radius: 12px;\n    box-shadow: 0 4px 15px rgba(0,0,0,0.5);\n}\n\n.close-btn {\n    position: absolute;\n    top: -10px;\n    right: -10px;\n    background: var(--danger-color);\n    color: white;\n    border: none;\n    border-radius: 50%;\n    width: 30px;\n    height: 30px;\n    cursor: pointer;\n    font-size: 1.2rem;\n}\n\n/* Results */\n.result-section {\n    margin-top: 2rem;\n    animation: fadeInUp 0.8s ease-out;\n}\n\n.result-content {\n    display: flex;\n    flex-wrap: wrap;\n    gap: 2rem;\n    align-items: center;\n    justify-content: center;\n}\n\n.result-image-box {\n    position: relative;\n    flex: 1;\n    min-width: 300px;\n    border-radius: 12px;\n    overflow: hidden;\n    border: 1px solid var(--accent-color);\n}\n\n.result-image-box img {\n    width: 100%;\n    display: block;\n}\n\n.scan-line {\n    position: absolute;\n    top: 0;\n    left: 0;\n    width: 100%;\n    height: 2px;\n    background: var(--accent-color);\n    box-shadow: 0 0 10px var(--accent-color);\n    animation: scan 2s infinite linear;\n    opacity: 0.8;\n}\n\n.result-details {\n    flex: 1;\n    min-width: 300px;\n    text-align: left;\n}\n\n.status-badge {\n    display: inline-block;\n    padding: 0.5rem 1.5rem;\n    border-radius: 50px;\n    font-weight: 700;\n    text-transform: uppercase;\n    font-size: 1.2rem;\n    margin-bottom: 2rem;\n    background: rgba(255, 255, 255, 0.1);\n}\n\n.status-badge.normal {\n    background: rgba(0, 255, 136, 0.2);\n    color: var(--success-color);\n    border: 1px solid var(--success-color);\n}\n\n.status-badge.defected {\n    background: rgba(255, 51, 68, 0.2);\n    color: var(--danger-color);\n    border: 1px solid var(--danger-color);\n    box-shadow: 0 0 20px rgba(255, 51, 68, 0.3);\n}\n\n.confidence-meter {\n    margin-bottom: 1.5rem;\n}\n\n.meter-bar {\n    height: 10px;\n    background: rgba(255, 255, 255, 0.1);\n    border-radius: 5px;\n    margin: 0.5rem 0;\n    overflow: hidden;\n}\n\n.meter-fill {\n    height: 100%;\n    background: linear-gradient(90deg, var(--accent-color), #ff8800);\n    width: 0%;\n    transition: width 1s ease-out;\n}\n\n.maintenance-msg {\n    font-size: 1.1rem;\n    line-height: 1.6;\n}\n\nfooter {\n    text-align: center;\n    margin-top: 4rem;\n    color: rgba(255, 255, 255, 0.4);\n    font-size: 0.9rem;\n}\n\n.hidden {\n    display: none !important;\n}\n\n/* Spinner */\n.spinner {\n    width: 20px;\n    height: 20px;\n    border: 3px solid rgba(255, 255, 255, 0.3);\n    border-radius: 50%;\n    border-top-color: white;\n    animation: spin 1s linear infinite;\n    margin-left: 10px;\n}\n\n@keyframes spin { to { transform: rotate(360deg); } }\n@keyframes scan { \n    0% { top: 0%; }\n    50% { top: 100%; }\n    100% { top: 0%; }\n}\n@keyframes fadeInDown { from { opacity: 0; transform: translateY(-20px); } to { opacity: 1; transform: translateY(0); } }\n@keyframes fadeInUp { from { opacity: 0; transform: translateY(20px); } to { opacity: 1; transform: translateY(0); } }\n\n\n\n\nINDEX.HTML\n\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>SolarGuard - Defect Detection</title>\n    <link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\">\n    <link href=\"https://fonts.googleapis.com/css2?family=Outfit:wght@300;400;600;700&display=swap\" rel=\"stylesheet\">\n</head>\n<body>\n    <div class=\"background-orb orb-1\"></div>\n    <div class=\"background-orb orb-2\"></div>\n\n    <div class=\"container\">\n        <header>\n            <div class=\"logo\">\n                <span class=\"icon\">‚òÄÔ∏è</span>\n                <h1>SolarGuard</h1>\n            </div>\n            <p class=\"subtitle\">Advanced Electroluminescence Analysis</p>\n        </header>\n\n        <main>\n            <div class=\"glass-card upload-section\">\n                <h2>Analyze Solar Panel</h2>\n                <form id=\"upload-form\">\n                    <div class=\"drop-zone\" id=\"drop-zone\">\n                        <span class=\"drop-icon\">üìÇ</span>\n                        <p>Drag & Drop EL Image here or <strong>Browse</strong></p>\n                        <input type=\"file\" name=\"file\" id=\"file-input\" accept=\"image/*\" hidden>\n                    </div>\n                    <div id=\"preview-container\" class=\"preview-container hidden\">\n                        <img id=\"image-preview\" src=\"\" alt=\"Preview\">\n                        <button type=\"button\" id=\"remove-file\" class=\"close-btn\">√ó</button>\n                    </div>\n                    <button type=\"submit\" class=\"btn-primary\" id=\"analyze-btn\" disabled>\n                        <span>Analyze Panel</span>\n                        <div class=\"spinner hidden\" id=\"spinner\"></div>\n                    </button>\n                </form>\n            </div>\n\n            <div id=\"result-section\" class=\"glass-card result-section hidden\">\n                <h2>Analysis Result</h2>\n                <div class=\"result-content\">\n                    <div class=\"result-image-box\">\n                        <img id=\"result-image\" src=\"\" alt=\"Analyzed Panel\">\n                        <div class=\"scan-line\"></div>\n                    </div>\n                    <div class=\"result-details\">\n                        <div class=\"status-badge\" id=\"status-badge\">Processing...</div>\n                        <div class=\"confidence-meter\">\n                            <span>Confidence Score</span>\n                            <div class=\"meter-bar\">\n                                <div class=\"meter-fill\" id=\"meter-fill\" style=\"width: 0%\"></div>\n                            </div>\n                            <span id=\"confidence-value\" class=\"confidence-value\">0%</span>\n                        </div>\n                        <p id=\"maintenance-msg\" class=\"maintenance-msg\"></p>\n                    </div>\n                </div>\n            </div>\n        </main>\n\n        <footer>\n            <p>&copy; 2025 SolarGuard AI. Built for Enhanced Energy Efficiency.</p>\n        </footer>\n    </div>\n\n    <script src=\"{{ url_for('static', filename='script.js') }}\"></script>\n</body>\n</html>\n\n\n\nAPP.PY\n\nimport os\nimport cv2\nimport numpy as np\nfrom flask import Flask, render_template, request, jsonify, url_for\nfrom ultralytics import YOLO\nfrom werkzeug.utils import secure_filename\n\napp = Flask(__name__)\n# Config\nUPLOAD_FOLDER = 'static/uploads'\nMODEL_PATH = '../models/elpv_yolov8/weights/best.pt' # Path to best model after training\n# Fallback if training not done, use pretrained (won't be accurate for solar, but works for code)\nDEFAULT_MODEL = 'yolov8n-cls.pt' \n\nos.makedirs(os.path.join(app.root_path, UPLOAD_FOLDER), exist_ok=True)\n\nprint(\"Loading model...\")\ntry:\n    if os.path.exists(MODEL_PATH):\n        model = YOLO(MODEL_PATH)\n        print(f\"Loaded trained model from {MODEL_PATH}\")\n    else:\n        print(f\"Warning: Trained model not found at {MODEL_PATH}. Using {DEFAULT_MODEL} placeholder.\")\n        model = YOLO(DEFAULT_MODEL)\nexcept Exception as e:\n    print(f\"Error loading model: {e}\")\n    model = None\n\n@app.route('/')\ndef index():\n    return render_template('index.html')\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    if 'file' not in request.files:\n        return jsonify({'error': 'No file part'}), 400\n    \n    file = request.files['file']\n    if file.filename == '':\n        return jsonify({'error': 'No selected file'}), 400\n    \n    if file:\n        filename = secure_filename(file.filename)\n        filepath = os.path.join(app.root_path, UPLOAD_FOLDER, filename)\n        file.save(filepath)\n        \n        # Inference\n        if model:\n            results = model(filepath)\n            # Classification user case\n            # results[0].probs.top1 -> index\n            # results[0].names -> dict\n            \n            # Get top prediction\n            probs = results[0].probs\n            top1_index = probs.top1\n            confidence = float(probs.top1conf)\n            class_name = results[0].names[top1_index]\n            \n            # Map class_name manually if using pretrained placeholder to avoid \"goldfish\"\n            # But if using our trained model, names should be 'normal', 'defected'\n            \n            result_data = {\n                'class': class_name,\n                'confidence': f\"{confidence:.2f}\",\n                'image_url': url_for('static', filename=f'uploads/{filename}')\n            }\n            return jsonify(result_data)\n        else:\n            return jsonify({'error': 'Model not loaded'}), 500\n\nif __name__ == '__main__':\n    app.run(debug=True, port=5000)\n\n\nIMPLEMENTING PLAN\n\nEnhancing Defect Classification in Solar Panels using EL Imaging and ML\nGoal Description\nAutomatically detect and classify solar panel defects using Electroluminescence (EL) images and the YOLOv8 deep learning model. The system will be accessible via a user-friendly web application.\n\nUser Review Required\nIMPORTANT\n\nDataset Acquisition: We need to confirm if the ELPV dataset can be automatically downloaded or if manual placement is required. Compute Resources: Training YOLOv8 requires reasonable compute (GPU recommended).\n\nProposed Changes\nProject Structure\ndata/: Directory for ELPV dataset (raw and processed)\nsrc/: Source code for preprocessing and training\napp/: Web application code (templates, static, backend)\nmodels/: Directory to save trained YOLOv8 models\nDependencies\nultralytics (YOLOv8)\nflask (Web Framework)\nopencv-python, numpy, pandas, scikit-learn (Data processing)\ngit (to clone dataset)\nData Preprocessing (src/preprocess.py)\nClone dataset from https://github.com/zae-bayern/elpv-dataset.\nLoad images and labels (labels are probabilities 0.0-1.0).\nBinarize labels: 0.0 = Normal, 1.0 (or >0.5) = Defected.\nResize images to 640x640 (standard for YOLOv8).\nNormalize pixel values.\nSplit into Train (70%), Val (20%), Test (10%).\nOrganize into YOLOv8 Classification format:\nroot/train/normal, root/train/defected\nroot/val/normal, root/val/defected\nroot/test/normal, root/test/defected\nModel Training (src/train.py)\nLoad yolov8n-cls.pt (Classification model).\nTrain on the processed folder.\nExport best model.\nWeb Application (app/)\nBackend: Flask app to load the model.\nFrontend: HTML/CSS/JS with \"Premium\" design (Dark mode, glassmorphism).\nFlow: Upload -> Process -> Show Result (Class + Confidence).\nVerification Plan\nAutomated Tests\nRun src/train.py --dry-run (few epochs) to ensure pipeline works.\nTest Flask endpoints with sample images.\nManual Verification\nUpload test images to the web app and verify output.\n\n\nWALKTHROUGH\n\nWalkthrough: Solar Panel Defect Classification\nThis document explains how to run the project from start to finish.\n\n1. Data Preprocessing (Run First)\nFile: \nsrc/preprocess.py\n Command: python src/preprocess.py\n\nWhat it does:\nLoads the ELPV dataset labels.\nSplits images into Training (70%), Validation (20%), and Testing (10%) sets.\nResizes all images to 640x640 pixels (standard for YOLOv8).\nSaves organized data into data/processed/.\n2. Model Training (Run Second)\nFile: src/train.py Command: python src/train.py\n\nWhat it does:\nLoads the yolov8n-cls.pt (Nano Classification) model.\nTrains the model on the processed data for 50 epochs.\nSaves the best model weights to models/elpv_yolov8/weights/best.pt.\nNote: Training can take time. If you want to skip this, the app will use a default pretrained model (less accurate for this specific task).\n3. Web Application (Run Last)\nFile: app/app.py Command: python app/app.py\n\nWhat it does:\nStarts the Flask web server.\nLoads the trained model (or fallback).\nOpens the interface at http://127.0.0.1:5000.\nAllows you to upload EL images and view defect predictions.\n4. Helper Files\nverify.py: a script to quickly test the API without opening the browser.\n\n\nTRAIN.PY\n\nfrom ultralytics import YOLO\n\ndef train():\n    # Load model\n    model = YOLO('yolov8n-cls.pt')  # load a pretrained model (recommended for training)\n\n    # Train\n    # data argument for classify mode is the path to folder containing train/val/test folders\n    results = model.train(data='data/processed', epochs=50, imgsz=640, project='models', name='elpv_yolov8')\n\n    # Validate\n    metrics = model.val()\n    print(\"Validation Accuracy:\", metrics.top1)\n\n    # Export\n    path = model.export(format='onnx')\n    print(f\"Model exported to {path}\")\n\nif __name__ == '__main__':\n    train()\n","setup_steps":[],"technologies":["Python","scikit-learn","FastAPI","pandas","Flask","HTML","Raspberry Pi","NumPy"],"stars":0,"forks":0,"last_updated":"2025-12-21T13:15:19Z","created_at":"2025-12-09T10:04:00Z","default_branch":"main"},{"name":"Automated-pest-disease-detection-for-agriculture","department":"AI","title":"Automated Pest Disease Detection For Agriculture","description":"No description available","repo_url":"https://github.com/Universal-college-projects/Automated-pest-disease-detection-for-agriculture","demo_url":null,"languages":["HTML","Python"],"topics":[],"difficulty":"beginner","estimated_time":"Varies","readme_content":"üå± Automated Pest & Disease Detection for Agriculture\n------------------------------------------------------------------------------------------------------------------------------------------------------\nThis project is a Machine Learning‚Äìbased web application designed to automatically detect pests and plant diseases from crop images, helping farmers and agricultural experts take early preventive action and improve crop yield.\n\nThe system uses a trained deep learning model (TensorFlow/Keras) to analyze uploaded plant images and predict the presence of pests or diseases through a simple Flask-based web interface.\n\nüöÄ Project Overview\n------------------------------------------------------------------------------------------------------------------------------------------------------------\nAgriculture faces significant losses due to late identification of pests and plant diseases. This project addresses that challenge by providing an automated image-based detection system that:\n\nAccepts plant images from users\n\nProcesses images using a trained CNN model\n\nPredicts pest or disease presence\n\nDisplays results through a user-friendly web interface\n\nüõ†Ô∏è Technologies Used\n---------------------------------------------------------------------------------------------------------------------------------------------\nPython\n\nFlask ‚Äì Web framework\n\nTensorFlow / Keras ‚Äì Deep learning model\n\nNumPy ‚Äì Numerical computations\n\nPillow (PIL) ‚Äì Image processing\n\nHTML/CSS ‚Äì Frontend interface\n-------------------------------------------------------------------------------------------------------------------------------------------\nüìÅ Project Structure\nproject/\n‚îÇ\n‚îú‚îÄ‚îÄ app.py                # Flask application (main server file)\n‚îú‚îÄ‚îÄ model.h5              # Trained deep learning model\n‚îú‚îÄ‚îÄ train_model.py        # Model training script\n‚îú‚îÄ‚îÄ data_setup.py         # Dataset preprocessing utilities\n‚îú‚îÄ‚îÄ requirements.txt      # Project dependencies\n‚îú‚îÄ‚îÄ README.md             # Project documentation\n‚îú‚îÄ‚îÄ RUN_GUIDE.md          # Instructions to run the project\n‚îÇ\n‚îú‚îÄ‚îÄ dataset/              # Training and testing image dataset\n‚îÇ\n‚îú‚îÄ‚îÄ templates/\n‚îÇ   ‚îî‚îÄ‚îÄ index.html        # Web interface (image upload & results)\n‚îÇ\n‚îú‚îÄ‚îÄ static/               # Static files (CSS, images if any)\n‚îî‚îÄ‚îÄ Test/                 # Test samples\n\n‚öôÔ∏è How It Works\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------\nUser uploads a plant image through the web interface\n\nImage is preprocessed using Pillow and NumPy\n\nThe trained TensorFlow model (model.h5) analyzes the image\n\nThe system predicts whether the plant is healthy or affected\n\nResults are displayed on the web page\n\nüéØ Key Features\n--------------------------------------------------------------------------------------------------------------------------------------------------------------\nAutomated pest and disease detection\n\nDeep learning‚Äìbased image classification\n\nSimple and interactive web interface\n\nEasy to extend with more crop categories\n\nSuitable for real-world agricultural use\n\nüåæ Future Enhancements\n----------------------------------------------------------------------------------------------------------------------------------------------------------------\nAdd more crop and disease classes\n\nImprove model accuracy with larger datasets\n\nProvide treatment and prevention suggestions\n\nDeploy on cloud platforms for real-time usage\n\nMobile application Integration\n-------------------------------\nMy Requriements ---------------------------------\n1. I require a camera option within the project for live detection of plant diseases.\n2. The current output is quite small; I necessitate a multi-window output.\n3. The output will be enhanced and presented in detail.\n","setup_steps":[],"technologies":["Python","Keras","Flask","TensorFlow","HTML","NumPy"],"stars":0,"forks":0,"last_updated":"2025-12-21T09:50:15Z","created_at":"2025-12-10T05:54:04Z","default_branch":"main"},{"name":"grama-soil-express-app","department":"CSE","title":"Grama Soil Express App","description":"No description available","repo_url":"https://github.com/Universal-college-projects/grama-soil-express-app","demo_url":null,"languages":["Java"],"topics":[],"difficulty":"beginner","estimated_time":"Varies","readme_content":"üåæ Grama Soil Express\n---------------------\n\nGrama Soil Express is an Android-based agriculture support application developed to help farmers in rural areas by providing soil-related information, crop details, and centralized administration through a simple mobile platform.\n\n1Ô∏è‚É£ Abstract\n------------\n\nGrama Soil Express is an Android-based agricultural support system designed to assist farmers in rural and semi-rural areas by providing easy access to soil health information and agricultural guidance. Traditional soil testing methods are often expensive, time-consuming, and inaccessible to village farmers, leading to improper fertilizer usage and reduced crop productivity.\n\nThis project aims to bridge the gap between agriculture and digital technology by offering a user-friendly mobile application. The system consists of two main modules: User (Farmer) and Admin. Farmers can log in, manage their profile, and access soil and crop-related information, while administrators manage village lists and crop master data. The application is developed using Android Studio (Java & XML) with Firebase Authentication and Firebase Realtime Database, ensuring secure access and real-time data management.\n\n2Ô∏è‚É£ Working of the Project\n-------------------------\n\nThe working of Grama Soil Express follows a simple and structured flow:\n\nUser or Admin logs in using Firebase Authentication.\n\nAfter successful login, the system checks the user role from Firebase Realtime Database.\n\nBased on the role:\n\nUser (Farmer) is redirected to the Home screen.\n\nAdmin is redirected to the Admin Dashboard.\n\nFarmers can view their profile, land details, and agricultural information.\n\nAdmin can manage village names and crop lists centrally.\n\nAll data updates are reflected in real time using Firebase.\n\n3Ô∏è‚É£ UML Diagrams (9 Diagrams)\n----------------------------\n\nThe following UML diagrams are designed for the project:\n\nUse Case Diagram\n----------------\n<img width=\"1600\" height=\"657\" alt=\"image\" src=\"https://github.com/user-attachments/assets/cfde8079-20c2-482c-8ca9-f83b3498e639\" />\n\n\nClass Diagram\n-------------\n<img width=\"1026\" height=\"1600\" alt=\"image\" src=\"https://github.com/user-attachments/assets/303bb421-92a5-441b-8d8f-307640f9ac7e\" />\n\n\nSequence Diagram\n----------------\n<img width=\"1600\" height=\"689\" alt=\"image\" src=\"https://github.com/user-attachments/assets/762750ad-a6cb-416d-8bd4-8768406ceae6\" />\n\n\n\nActivity Diagram\n----------------\n<img width=\"633\" height=\"1600\" alt=\"image\" src=\"https://github.com/user-attachments/assets/51e743a9-fce4-41c6-9e70-011793da07fb\" />\n\n\nState Diagram\n------------\n<img width=\"1561\" height=\"1600\" alt=\"image\" src=\"https://github.com/user-attachments/assets/d2bf01b7-e18c-4048-b891-9d259cb2b9ba\" />\n\n\n\nComponent Diagram\n-----------------\n<img width=\"1600\" height=\"1208\" alt=\"image\" src=\"https://github.com/user-attachments/assets/7dc0a3eb-5e17-47de-8bba-50dac8a01dd9\" />\n\n\n\nDeployment Diagram\n------------------\n<img width=\"1600\" height=\"1084\" alt=\"image\" src=\"https://github.com/user-attachments/assets/f155e032-9232-459e-b51a-7b62773ef014\" />\n\n\n\nData Flow Diagram (DFD ‚Äì Level 0)\n--------------------------------\n<img width=\"1600\" height=\"271\" alt=\"image\" src=\"https://github.com/user-attachments/assets/98b27f85-1ca4-4f6e-8442-9aeb437efe10\" />\n\n\n\nData Flow Diagram (DFD ‚Äì Level 1)\n---------------------------------\n<img width=\"1600\" height=\"748\" alt=\"image\" src=\"https://github.com/user-attachments/assets/3600fdfa-4cdb-4d96-8be6-83bd1e56f84c\" />\n\n\n\nThese diagrams explain the system structure, data flow, and interaction between users, admin, and the system.\n\n4Ô∏è‚É£ Results Images\n-----------------\nThe results section includes screenshots of the implemented application such as:\n\nLogin Screen\n------------\n<img width=\"540\" height=\"1204\" alt=\"image\" src=\"https://github.com/user-attachments/assets/575042c2-e632-4cda-8523-08ea3a64df6a\" />\n\nUser Home Screen\n----------------\n<img width=\"540\" height=\"1204\" alt=\"image\" src=\"https://github.com/user-attachments/assets/9f6018ef-4421-46ac-bec2-db93facfe1fd\" />\n\nUser Profile Screen\n-------------------\n<img width=\"540\" height=\"1204\" alt=\"image\" src=\"https://github.com/user-attachments/assets/b242b14c-cd0d-4e6a-a8a8-0b80a5070175\" />\n\nSettings Screen\n---------------\n<img width=\"540\" height=\"1204\" alt=\"image\" src=\"https://github.com/user-attachments/assets/b054951a-aad5-4e0a-93ac-fa3534c9ff00\" />\n\nAdmin Dashboard\n---------------\n<img width=\"540\" height=\"1204\" alt=\"image\" src=\"https://github.com/user-attachments/assets/508d66bd-0215-49b1-a413-1ccdbac91ed3\" />\n\nVillage Management Screen\n------------------------\n<img width=\"540\" height=\"1204\" alt=\"image\" src=\"https://github.com/user-attachments/assets/4d643b99-a3e0-4be8-bcce-fd3b0763aedb\" />\n\nCrop Management Screen\n----------------------\n<img width=\"540\" height=\"1204\" alt=\"image\" src=\"https://github.com/user-attachments/assets/918638b8-c756-47cc-8db8-7cd35698dc1d\" />\n\nThese images demonstrate the successful implementation of role-based login and admin-controlled data management.\n\n5Ô∏è‚É£ Training Images\n------------------\n\nTraining images include:\n\nUI design screenshots\n---------------------\n<img width=\"442\" height=\"113\" alt=\"image\" src=\"https://github.com/user-attachments/assets/3bbcd82b-7a51-463c-ab0f-34ad69e7d2e3\" />\n\n\nFirebase Authentication setup\n----------------------------\n<img width=\"1420\" height=\"734\" alt=\"Screenshot 2025-12-20 095149\" src=\"https://github.com/user-attachments/assets/06967a31-a390-4dd9-a59a-f1a5e25c1006\" />\n\n\nFirebase Realtime Database structure\n------------------------------------\n<img width=\"1914\" height=\"838\" alt=\"Screenshot 2025-12-18 120757\" src=\"https://github.com/user-attachments/assets/2de8b107-6499-4188-8522-6491192dcbd8\" />\n<img width=\"1434\" height=\"673\" alt=\"Screenshot 2025-12-20 095302\" src=\"https://github.com/user-attachments/assets/55001451-f7c2-4253-b9a9-018f07ff9575\" />\n\nAdmin data entry screens\n-----------------------\n<img width=\"1032\" height=\"445\" alt=\"Screenshot 2025-12-17 025558\" src=\"https://github.com/user-attachments/assets/e7269207-9452-4a3d-8969-e6cf7a7fd6cc\" />\n\n\nThese images show how the application was configured and tested during development.\n\n6Ô∏è‚É£ Software Tools Used\n----------------------\n\nThe following software tools were used in the development of this project:\n\nAndroid Studio ‚Äì Application development\n\nJava ‚Äì Application logic\n\nXML ‚Äì User Interface design\n\nFirebase Console ‚Äì Backend services\n\nGit & GitHub ‚Äì Version control\n\nWindows OS ‚Äì Development environment\n\n7Ô∏è‚É£ Code Libraries Used\n----------------------\n\nThe following libraries and services are used in the project:\n\nFirebase Authentication\n\nFirebase Realtime Database\n\nAndroidX Libraries\n\nMaterial Design Components\n\nConstraintLayout\n\nRecyclerView\n\nüìå Conclusion\n-------------\n\nGrama Soil Express demonstrates how mobile and cloud technologies can be effectively used to support agriculture and rural development. The project provides a scalable foundation that can be extended with soil testing hardware, AI-based recommendations, and multilingual support in the future.\n","setup_steps":[],"technologies":["Java","Express"],"stars":0,"forks":0,"last_updated":"2025-12-21T03:18:17Z","created_at":"2025-12-09T09:54:16Z","default_branch":"main"},{"name":"real-time-learning-assistant","department":"CSE","title":"Real Time Learning Assistant","description":"No description available","repo_url":"https://github.com/Universal-college-projects/real-time-learning-assistant","demo_url":null,"languages":["HTML","JavaScript","CSS"],"topics":[],"difficulty":"intermediate","estimated_time":"Varies","readme_content":"Title: REAL TIME LEARNING ASSISTANT\r\n\r\nThe Real Time Learning Assistant is an interactive web-based platform designed to support students in preparing for competitive examinations and academic assessments through modern digital learning tools. The system offers a structured, user-friendly, and highly accessible online environment where learners can practice previous year papers, attempt mock tests, explore exam-specific syllabi, and receive instant feedback to improve their performance.\r\n\r\nThe platform aims to solve the challenges faced by students in exam preparation‚Äîsuch as lack of proper guidance, limited availability of quality study materials, and absence of personalized learning support. By integrating real-time interaction, performance tracking, and adaptive learning components, the Real Time Learning Assistant ensures that students prepare efficiently, smartly, and confidently.\r\n\r\nPurpose of the System\r\n\r\nThe primary purpose of the Real Time Learning Assistant is to offer a centralized, real-time, and adaptive learning environment that helps students enhance their knowledge, practice effectively, and track progress continuously. The system also ensures flexibility, allowing learners to access materials from anywhere at any time.\r\n\r\n\r\n\r\n\r\nImplemented Screens:\r\n\r\nDashboard - Clean welcome screen with exam categories (RRB, SSC, GATE), quick-access buttons, performance summary with progress bars for strong/weak topics\r\n\r\n\r\nPrevious Year Papers - Card-based layout with filters for exam type, year, and difficulty\r\n\r\n\r\nMock Test Interface - Professional exam environment with timer, question navigation panel, and progress tracking\r\n\r\n\r\nSyllabus Viewer - Collapsible sections for different subjects with progress tracking and search functionality\r\n\r\n\r\nDaily Quiz - 5-question daily challenge with leaderboard and streak tracking\r\n\r\n\r\n\r\nFlashcards - Interactive flip cards for formulas and definitions with category selection\r\n\r\nBookmarked Questions - Grid layout of saved questions with comprehensive filtering options\r\n\r\nProfile & Dashboard Summary - User profile with achievements, badges, stats, and recent activity\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nMISSING CONTENT OR CHANGES REQUIRED:\r\n\r\nChanges in Student portal:\r\n\r\n->DASHBOARD\r\n\r\n>In dashboard progress of student is not updating\r\n\r\n>dark theme and light theme not working\r\n\r\n>recent activities not storing or updating\r\n\r\n>change \"welcome back student\" to \"welcome back -username-\"(name of the user should be displayed)\r\n \r\n->MOCK TEST\r\n\r\n>For each question in mock test the option to bookmark the question should be added(if the student feels difficulty he saves the question in bookmark so that he can revisit the question in bookmark page and practice it)\r\n\r\n>In mock test each question should carry one minute\r\n\r\n>User selects course (RRB / SSC / GATE)\r\n\r\nUser selects subtopic (Aptitude / Reasoning / English)\r\n\r\nif I click aptitude i should be able to add 10 subtopics\r\n\r\nfor each sub topic 10 questions should be stored \r\n\r\ntotal 100 questions stored in RRB sub topic aptitude which contains more 10 subtopics that makes it 100 questions\r\n\r\nfor mock test System randomly picks 10 questions from these selected overall subtopics\r\n\r\nonly ten questions for each mock test\r\n\r\n>remove mark for review button in mock test\r\n\r\n>In mock test page -> overview change the design to jump to question\r\n\r\n>User -> course -> syllabus ->in syllabus navigation links inside sidebar not working\r\n\r\n>In mock tests the questions should be taken from the file(.json)that admin is going to upload that to 10 questions randomly from given sub-topics from the courses\r\n\r\n>if a user wants to take mock test multiple times the questions should be randomly generated each time\r\n\r\n->DAILY-QUIZ\r\n\r\n>In daily-quiz the questions should be taken from the file that admin is going to upload that to 5 questions randomly from given the file\r\n\r\n>3 flash cards should be generated randomly, daily from the given file from admin\r\n\r\n\r\n->ADMIN\r\n\r\n>Total number of users is not displaying\r\n\r\n>Active tests is not working properly\r\n\r\n>Remove feedback function\r\n\r\n>removing or deleting a question or a course or a subtopic is not working, so add an option to delete a single question or a course or a sub topic\r\n\r\n>Mock tests and daily quizzes questions should be taken from the files given by the admin\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n","setup_steps":[],"technologies":["CSS","HTML","JavaScript"],"stars":0,"forks":0,"last_updated":"2025-12-20T15:52:35Z","created_at":"2025-12-09T09:57:16Z","default_branch":"main"},{"name":"comprehensive-automated-doc-verification","department":"CSE","title":"Comprehensive Automated Doc Verification","description":"No description available","repo_url":"https://github.com/Universal-college-projects/comprehensive-automated-doc-verification","demo_url":null,"languages":["Python"],"topics":[],"difficulty":"intermediate","estimated_time":"Varies","readme_content":"# CertiSecure - Secure Marksheet Generator & Validator\r\n\r\nCertiSecure is a web application designed to generate tamper-proof marksheet PDFs and validate them using cryptographic hashing and strict content verification.\r\n\r\n## Features\r\n\r\n### 1. Secure Generation\r\n-   **Data Hashing**: Creates a SHA-256 hash of the student's text details (Name, Marks, etc.).\r\n-   **Image Hashing**: Normalizes the profile photo to JPEG and calculates its SHA-256 hash to prevent format-based verification errors.\r\n-   **QR Code Embedding**: Embeds a QR code containing the digital signature (hashes) of both the text and the image.\r\n-   **Visual Security**: Adds a visual border to the profile photo and embeds the specific JPEG stream that was hashed.\r\n\r\n### 2. Strict Validation\r\n-   **Content Extraction**: Extracts raw text and images from the uploaded PDF.\r\n-   **Integrity Check**: Re-calculates hashes of the extracted content and compares them byte-for-byte with the data stored in the QR code.\r\n-   **Visual Tamper Detection**: Automatically fails validation if any PDF annotations or overlays (common in \"fill and sign\" editors) are detected.\r\n-   **Feedback**: Provides detailed pass/fail reports for text, photo, and visual layer integrity.\r\n\r\n## Prerequisites\r\n\r\n-   Python 3.8+\r\n-   Pip (Python Package Manager)\r\n\r\n## Installation Guide\r\n\r\n1.  **Clone/Download** this project folder.\r\n2.  **Open a terminal** inside the project folder.\r\n3.  **Install Dependencies**:\r\n    ```bash\r\n    pip install -r requirements.txt\r\n    ```\r\n\r\n## Step-by-Step Usage\r\n\r\n### Starting the Server\r\nRun the application using Python:\r\n```bash\r\npython app.py\r\n```\r\nThe server will start at `http://127.0.0.1:5000`.\r\n\r\n### Generating a Marksheet\r\n1.  Open your browser and go to `http://127.0.0.1:5000`.\r\n2.  Fill in the form (Name, Father's Name, Institution, Roll No, Marks).\r\n3.  Upload a Profile Photo (JPG/PNG).\r\n4.  Click **Generate Secured PDF**.\r\n5.  The secured PDF will run and download automatically.\r\n\r\n### Validating a Marksheet\r\n1.  Go to the **Validate Document** section on the home page.\r\n2.  Upload a Marksheet PDF (either the original or a generic file).\r\n3.  Click **Run Verification**.\r\n4.  **Check the Results**:\r\n    -   **Authentic**: All hashes match, and no tampering detected.\r\n    -   **Invalid / Tampered**: The text or photo has been changed, or someone added visual annotations.\r\n\r\n## Technical Notes\r\n-   **Image Handling**: To ensure consistency, all uploaded photos are converted to JPEG quality 95 before hashing. This ensures the hash stored in the QR always matches the image embedded in the PDF.\r\n-   **Libraries Used**: \r\n    -   `Flask` (Web Framework)\r\n    -   `ReportLab` (PDF Generation)\r\n    -   `PyMuPDF (fitz)` (PDF Reading/Extraction)\r\n    -   `OpenCV` (QR Code Detection)\r\n    -   `Pillow` (Image Processing)\r\n\r\nAutomated Document Verification System for Official Documentation - https://1drv.ms/w/c/87C32D068B13D74E/IQC60j2_IM9eRa21Bq-kq1lLAemn0T-eCkr7fRg1lZ1jG6E?e=SjqEtO\r\n","setup_steps":["**Clone/Download** this project folder.","**Open a terminal** inside the project folder.","**Install Dependencies**:"],"technologies":["Flask","Python"],"stars":0,"forks":0,"last_updated":"2025-12-20T15:50:03Z","created_at":"2025-12-09T09:53:17Z","default_branch":"main"},{"name":"Deep-Learning-Techniques-for-Diabetic-Retinopathy-classification-","department":"AI","title":"Deep Learning Techniques For Diabetic Retinopathy Classification ","description":"No description available","repo_url":"https://github.com/Universal-college-projects/Deep-Learning-Techniques-for-Diabetic-Retinopathy-classification-","demo_url":null,"languages":["Jupyter Notebook","Python"],"topics":[],"difficulty":"advanced","estimated_time":"Varies","readme_content":"# **Deep Learning Techniques for Diabetic Retinopathy Classification**\n1. Introduction to Diabetic Retinopathy (DR)\n\nDiabetic Retinopathy is a diabetes-related eye disease caused by damage to retinal blood vessels. It progresses through stages:\n\nNo DR\n\nMild Non-Proliferative DR\n\nModerate Non-Proliferative DR\n\nSevere Non-Proliferative DR\n\nProliferative DR\n\nEarly detection is crucial to prevent blindness, and deep learning has shown high accuracy in automated DR screening using retinal fundus images.\n\n2. Why Deep Learning for DR Classification?\n\nTraditional image processing methods struggle with:\n\nVariations in illumination\n\nNoise and low contrast\n\nSmall lesions (microaneurysms)\n\nDeep Learning excels because it:\n\nAutomatically learns hierarchical features\n\nHandles complex patterns\n\nScales well with large datasets\n\n3. Commonly Used Datasets\nDataset\tDescription\nEyePACS\tLarge-scale Kaggle dataset, 5 DR classes\nAPTOS 2019\tHigh-quality fundus images\nMessidor\tClinical-grade dataset\nIDRiD\tIncludes lesion-level annotations\n4. Image Preprocessing Techniques\n\nPreprocessing improves model performance significantly:\n\nImage resizing (224√ó224 or 512√ó512)\n\nContrast Limited Adaptive Histogram Equalization (CLAHE)\n\nNoise removal\n\nData augmentation:\n\nRotation\n\nFlipping\n\nZoom\n\nBrightness adjustment\n\n5. Deep Learning Models Used\n5.1 Convolutional Neural Networks (CNNs)\n\nCNNs are the backbone of DR classification.\n\nPopular CNN Architectures:\n\nVGG16 / VGG19\n\nResNet50 / ResNet101\n\nDenseNet121\n\nInceptionV3\n\nEfficientNet (B0‚ÄìB7)\n\n‚úÖ EfficientNet is widely used due to:\n\nBetter accuracy\n\nFewer parameters\n\nHigh computational efficiency\n\n5.2 Transfer Learning\n\nInstead of training from scratch:\n\nUse pretrained ImageNet weights\n\nFine-tune last layers\n\nAdvantages:\n\nFaster convergence\n\nHigher accuracy\n\nRequires less data\n\n5.3 Vision Transformers (ViTs)\n\nRecent models use attention mechanisms.\n\nExamples:\n\nVision Transformer (ViT)\n\nSwin Transformer\n\nAdvantages:\n\nCaptures global context\n\nPerforms well on high-resolution images\n\nLimitations:\n\nRequires more data\n\nComputationally expensive\n\n5.4 Hybrid CNN + Transformer Models\n\nCombines:\n\nCNN for local feature extraction\n\nTransformer for global dependencies\n\nExample:\n\nCNN backbone + Attention layer\n\n6. Classification Approaches\n6.1 Binary Classification\n\nDR vs No DR\n\n6.2 Multi-Class Classification\n\n5-stage DR classification\n\n6.3 Lesion-Based Detection\n\nMicroaneurysms\n\nHemorrhages\n\nExudates\n\n(Used in advanced diagnostic systems)\n\n7. Loss Functions and Optimization\n\nCategorical Cross-Entropy\n\nFocal Loss (handles class imbalance)\n\nAdam / AdamW optimizer\n\nLearning rate scheduling\n\n8. Evaluation Metrics\nMetric\tImportance\nAccuracy\tOverall correctness\nPrecision\tFalse positive control\nRecall (Sensitivity)\tCritical for medical diagnosis\nF1-Score\tBalance of precision & recall\nAUC-ROC\tModel reliability\n\n‚ö†Ô∏è High recall is preferred to avoid missing DR cases.\n\n9. Challenges in DR Classification\n\nClass imbalance\n\nImage quality variations\n\nSmall lesion detection\n\nOverfitting on limited datasets\n\n10. Future Research Directions\n\nSelf-supervised learning\n\nExplainable AI (Grad-CAM, LIME)\n\nMulti-modal learning (images + patient data)\n\nLightweight models for mobile screening\n\nReal-time deployment in rural healthcare\n\n11. Conclusion\n\nDeep learning, particularly CNNs and Transformer-based models, has revolutionized Diabetic Retinopathy classification. With proper preprocessing, transfer learning, and evaluation, automated DR systems can achieve expert-level performance, enabling scalable and affordable screening.\n","setup_steps":[],"technologies":["Python","Jupyter Notebook"],"stars":1,"forks":0,"last_updated":"2025-12-20T10:33:06Z","created_at":"2025-12-09T09:59:47Z","default_branch":"main"},{"name":"Audio-data-preparation-and-argument-using-tensorflow-","department":"AI","title":"Audio Data Preparation And Argument Using Tensorflow ","description":"No description available","repo_url":"https://github.com/Universal-college-projects/Audio-data-preparation-and-argument-using-tensorflow-","demo_url":null,"languages":["Python"],"topics":[],"difficulty":"advanced","estimated_time":"Varies","readme_content":"# Audio Data Preparation, Augmentation & Classification Studio\r\n\r\nA comprehensive web application for analyzing, augmenting, and classifying audio data using **TensorFlow**, **YAMNet**, and **Flask**.\r\n\r\n## üöÄ Features\r\n\r\n### 1. Audio Ingestion\r\n- **File Upload**: Support for `.wav` file uploads.\r\n- **Microphone Recording**: Built-in browser recorder that captures audio, converts it to WAV client-side, and sends it for analysis.\r\n\r\n### 2. AI Classification (YAMNet)\r\n- Integrates Google's **YAMNet** model.\r\n- Automatically identifies audio events from **521 classes** (e.g., \"Speech\", \"Music\", \"Clapping\", \"Dog\", \"Siren\").\r\n- Displays top 5 predictions with confidence scores.\r\n\r\n### 3. Advanced Visualization Pipeline\r\nThe app processes audio through a multi-stage pipeline and visualizes each step:\r\n- **Waveform**: Raw time-domain signal.\r\n- **Trimming**: Automatic silence removal.\r\n- **Fading**: Logarithmic fade-in/out application.\r\n- **Spectrograms**:\r\n    - Standard Spectrogram (Log Scale).\r\n    - Mel Spectrogram (Perceptually relevant scale).\r\n    - DB-Scaled Mel Spectrogram.\r\n- **Augmentations**:\r\n    - **Frequency Masking**: Randomly masks frequency bands (useful for training robust models).\r\n    - **Time Masking**: Randomly masks time steps.\r\n\r\n## üõ†Ô∏è Tech Stack\r\n\r\n- **Backend**: Python, Flask\r\n- **ML/AI**: TensorFlow, TensorFlow I/O, TensorFlow Hub\r\n- **Audio Processing**: `tensorflow-io`, `soundfile`, `numpy`\r\n- **Frontend**: HTML5, CSS3 (Dark Mode), JavaScript (Web Audio API)\r\n\r\n## üìã Prerequisites\r\n\r\n- Python 3.9+\r\n- Internet connection (for initial model download)\r\n\r\n## ‚ö° Installation\r\n\r\n1.  **Clone/Download** the project.\r\n2.  **Install Dependencies**:\r\n    ```bash\r\n    pip install -r requirements.txt\r\n    ```\r\n\r\n## üñ•Ô∏è Usage\r\n\r\n1.  **Start the Server**:\r\n    Run the following command in your terminal:\r\n    ```bash\r\n    python app.py\r\n    ```\r\n    *Note: The first run may take a moment to download the YAMNet model.*\r\n\r\n2.  **Open Dashboard**:\r\n    Go to [http://127.0.0.1:5000](http://127.0.0.1:5000) in your browser.\r\n\r\n3.  **Analyze Audio**:\r\n    - **Upload**: Choose a `.wav` file and click **\"Upload & Process\"**.\r\n    - **Record**: Click **\"üî¥ Record\"**, speak or make a sound, then click **\"‚èπ Stop\"**.\r\n\r\n## üìÇ Project Structure\r\n\r\n```\r\n‚îú‚îÄ‚îÄ app.py                  # Main Flask application entry point\r\n‚îú‚îÄ‚îÄ audio_processing.py     # Core logic: TF pipeline, YAMNet classification\r\n‚îú‚îÄ‚îÄ requirements.txt        # Python dependencies\r\n‚îú‚îÄ‚îÄ README.md               # Documentation\r\n‚îú‚îÄ‚îÄ uploads/                # Directory for temporary audio storage\r\n‚îú‚îÄ‚îÄ static/\r\n‚îÇ   ‚îî‚îÄ‚îÄ images/             # Generated visualization plots\r\n‚îî‚îÄ‚îÄ templates/\r\n    ‚îî‚îÄ‚îÄ index.html          # Dashboard frontend with Recorder logic\r\n```\r\n\r\n## üîß Troubleshooting\r\n\r\n- **\"Format not recognised\" error**: Ensure you have refreshed the page to load the latest JavaScript recorder, which converts microphone input to WAV automatically.\r\n- **Model downloading**: If the app hangs on startup, check your internet connection; it needs to fetch the model from TF Hub.\r\n\r\n---\r\n*Created with the assistance of Google DeepMind's Antigravity Agent.*\r\n","setup_steps":[],"technologies":["TensorFlow","NumPy","Flask","Python"],"stars":0,"forks":0,"last_updated":"2025-12-20T10:29:58Z","created_at":"2025-12-10T08:54:52Z","default_branch":"main"},{"name":"patient-health-monitoring","department":"ECE","title":"Patient Health Monitoring","description":"No description available","repo_url":"https://github.com/Universal-college-projects/patient-health-monitoring","demo_url":null,"languages":["Python"],"topics":[],"difficulty":"beginner","estimated_time":"Varies","readme_content":"# Need Help in Documentation ..!\n \n # Patient Health Monitoring Using Portable Device with Microsoft Software\n\nüöÄ Overview\n\n A smart and portable IoT-based system that monitors patient health parameters in real time using sensor hardware integrated with Microsoft Azure cloud services. This solution helps doctors and caregivers track vital signs remotely, detect abnormalities early, and act quickly during emergencies.\n\n‚≠ê  Features\n\nü´Ä Real-time vital signs monitoring\n\nüì° Azure IoT Hub communication\n\nüìä Power BI real-time dashboard\n\nüîî Automatic health alerts\n\nüîí Secure cloud data storage\n\nüì± Remote access for doctors\n\nüíº Portable & easy-to-use device\n\nüß¨ Vital Signs Monitored\nParameter\t                      Sensor\n‚ù§Ô∏è Heart Rate\t               Pulse Sensor\nüî• Body Temperature \t     Temperature Sensor\nü´Å SpO‚ÇÇ\t                    MAX30100 / Pulse Oximeter\nüîå ECG\t                    ECG Sensor Module\nüíâ Blood Pressure\t         BP Sensor\nüèóÔ∏è Architecture\nü©∫ Sensors  \n     ‚Üì  \nüìü Microcontroller (ESP32 / NodeMCU)  \n     ‚Üì  \n‚òÅÔ∏è Azure IoT Hub  \n     ‚Üì  \n‚öôÔ∏è Azure Functions / Stream Analytics  \n     ‚Üì  \nüóÑÔ∏è Azure SQL / Blob Storage  \n     ‚Üì  \nüìä Power BI Dashboard\n\nüõ†Ô∏è Tech Stack\nHardware\n\nArduino / ESP32\n\nBiomedical sensors\n\nWiFi / BLE Module\n\nSoftware + Cloud\n\nMicrosoft Azure IoT Hub\n\nAzure Functions\n\nAzure SQL / Blob\n\nPower BI\n\nPython / Embedded C\n\nüìÅ Project Structure\nüì¶ Patient-Health-Monitoring\n‚îú‚îÄ‚îÄ hardware/\n‚îú‚îÄ‚îÄ software/\n‚îú‚îÄ‚îÄ cloud/\n‚îú‚îÄ‚îÄ images/\n‚îî‚îÄ‚îÄ README.md\n\nüß† How It Works\n\nSensors collect patient vitals\n\nMicrocontroller processes and uploads data\n\nAzure IoT Hub receives data streams\n\nAzure Functions analyze parameters\n\nPower BI displays real-time results\n\nAlerts are triggered for abnormal values\n\nüéØ Use Cases\n\nHome healthcare\n\nElderly monitoring\n\nRural health services\n\nHospital remote monitoring\n\nPost-surgery care\n\nü§ù Contributing\n\nPull requests, suggestions, and improvements are always welcome!**\n","setup_steps":[],"technologies":["Arduino","IoT","Python","ESP32"],"stars":0,"forks":0,"last_updated":"2025-12-20T08:36:57Z","created_at":"2025-12-09T10:05:37Z","default_branch":"main"},{"name":"smart-parking-","department":"ECE","title":"Smart Parking ","description":"No description available","repo_url":"https://github.com/Universal-college-projects/smart-parking-","demo_url":null,"languages":[],"topics":[],"difficulty":"intermediate","estimated_time":"Varies","readme_content":"[Out puts.docx](https://github.com/user-attachments/files/24269245/Out.puts.docx)\n[Uml diagrams.docx](https://github.com/user-attachments/files/24269244/Uml.diagrams.docx)\n# smart-parking-\n\nhttps://github.com/noorkhokhar99/car-parking-finder\n\n1. Project Title\n\nIoT-Based Smart Parking and Vehicle Occupancy Detection System with Mobile Application\n\n2. Project Description\n\nThe IoT-Based Smart Parking and Vehicle Occupancy System is designed to efficiently monitor parking spaces in real time and provide instant updates to users through a mobile application.\nThis system detects whether parking slots are occupied or vacant using sensors and image processing techniques, sends the data to a cloud server, and displays the live parking status on a mobile app.\n\nThe primary goal of the project is to reduce parking congestion, save time, and improve user convenience by allowing users to check parking availability before entering a parking area. The system also helps parking administrators manage parking spaces more effectively.\n\nThe system integrates IoT hardware, cloud services, computer vision (optional), and mobile application development to create a smart, scalable, and real-time parking solution.\n\n3. Project Objectives\n\nTo detect vehicle presence in parking slots accurately\n\nTo identify empty and occupied parking spaces in real time\n\nTo send live parking data to a cloud database\n\nTo display parking availability on a mobile application\n\nTo reduce traffic congestion and parking search time\n\nTo improve efficient utilization of parking spaces\n\n4. System Working (High-Level Flow)\n\nSensors or camera monitor each parking slot\n\nVehicle presence is detected (occupied / empty)\n\nData is sent to cloud server (Firebase / IoT platform)\n\nCloud processes and stores parking status\n\nMobile application fetches real-time data\n\nUser views available parking slots on the app\n\n5. Functional Requirements\nA. Parking Slot Detection\n\nDetect vehicle presence in each slot\n\nUpdate slot status automatically (Empty / Occupied)\n\nSupport multiple parking slots\n\nB. Cloud Data Management\n\nStore real-time parking data\n\nUpdate data instantly when slot status changes\n\nEnsure reliable data synchronization\n\nC. Mobile Application\n\nDisplay live parking availability\n\nShow number of empty and occupied slots\n\nRefresh data automatically\n\nSimple and user-friendly interface\n\n6. Hardware Requirements\nComponent\tDescription\nUltrasonic / IR Sensors\tDetect vehicle presence\nCamera (Optional)\tFor image-based detection\nMicrocontroller\tESP32 / Arduino / Raspberry Pi\nWi-Fi Module\tInternet connectivity (ESP32 inbuilt)\nPower Supply\t5V / 12V regulated\nParking Slot Frame / Model\tPhysical demonstration setup\n7. Software Requirements\nSoftware\tPurpose\nAndroid Studio\tMobile app development\nFirebase Realtime Database\tCloud data storage\nFirebase Authentication (Optional)\tUser login\nPython / OpenCV\tImage processing (if camera-based)\nArduino IDE\tMicrocontroller programming\nVS Code\tBackend / Python development\n8. Technologies Used\n\nInternet of Things (IoT)\n\nCloud Computing (Firebase)\n\nMobile Application Development (Android)\n\nComputer Vision (Optional)\n\nWireless Communication (Wi-Fi)\n\n9. Advantages\n\nSaves time and fuel\n\nReduces traffic congestion\n\nReal-time parking updates\n\nUser-friendly mobile interface\n\nScalable for large parking areas\n\n#10. Applications\n\nShopping malls\n\nSmart cities\n\nAirports\n\nHospitals\n\nCorporate parking areas\n\nResidential complexes\n\n11. Future Enhancements\n\nSlot reservation feature\n\nNavigation to nearest empty slot\n\nPayment and billing system\n\nAI-based vehicle recognition\n\nLicense plate detection\n\n\n#smart parking document link---https://docs.google.com/document/d/1kwMrJE5IIZhQmdHM0dpo3ZQ2KgRRRpy4/edit?usp=sharing&ouid=104028342905149223028&rtpof=true&sd=true\n","setup_steps":[],"technologies":["IoT","ESP32","Arduino","Raspberry Pi"],"stars":0,"forks":0,"last_updated":"2025-12-20T06:57:26Z","created_at":"2025-12-09T09:55:45Z","default_branch":"main"},{"name":"Rag-knowledge-chat-boot","department":"AI","title":"Rag Knowledge Chat Boot","description":"No description available","repo_url":"https://github.com/Universal-college-projects/Rag-knowledge-chat-boot","demo_url":null,"languages":["Jupyter Notebook","Python"],"topics":[],"difficulty":"intermediate","estimated_time":"Varies","readme_content":"# rag-qa-self\nRAG based application on question and answer about myself\n\nI wrote about this in this article: [How I Built a RAG-based AI Chatbot from My Personal Data](https://medium.com/keeping-up-with-ai/how-i-built-a-rag-based-ai-chatbot-from-my-personal-data-88eec0d3483c)\n\n**Overview**\n\nThis AI-powered app, built with Streamlit, uses a Large Language Model (LLM) to answer questions based on PDF data about Ana. Key technologies include:\n\n- Langchain as the framework, \n- GPT4All for embedding generation, \n- Gemini 1.5 as the LLM, \n- Streamlit for the user interface, and \n- Chroma DB as the vector database.\n\n\ncreate the .pdf datasets\n\nbefore very vector db creation delete chroma folder \n\n**Instructions**\n\n1. run pip install -r requirements.txt\n2. generate your own GEMINI API KEY from https://aistudio.google.com/ and put it in .env file as GEMINI_API_KEY = \"YOUR_KEY\"\n3. run `python create_db.py` in the terminal to create the vector database from the documents.\n4. run `streamlit run main.py` to launch streamlit UI in the browser\n\n#rag\nThis project implements a Retrieval-Augmented Generation (RAG) chatbot designed to provide accurate, context-aware responses by combining Large Language Models (LLMs) with a domain-specific knowledge base. It leverages retrieval mechanisms to fetch relevant documents or information snippets and uses an LLM to generate natural, coherent answers grounded in the retrieved data.\n#road map \nA phased approach to build the RAG Knowledge Base Chatbot:\n\nPhase 1 ‚Äî Setup and Data Preparation\n\nPhase 2 ‚Äî Retrieval Pipeline\n\nPhase 3 ‚Äî Generation and Integration\n\nPhase 4 ‚Äî API and UI Development\n\nPhase 5 ‚Äî Testing, Polish,¬†and¬†Deployment\n","setup_steps":[],"technologies":["Python","Jupyter Notebook"],"stars":0,"forks":0,"last_updated":"2025-12-20T06:09:23Z","created_at":"2025-12-09T10:32:30Z","default_branch":"main"},{"name":"Ai-based-hr-screening-chatbot","department":"AI","title":"Ai Based Hr Screening Chatbot","description":"No description available","repo_url":"https://github.com/Universal-college-projects/Ai-based-hr-screening-chatbot","demo_url":null,"languages":[],"topics":[],"difficulty":"intermediate","estimated_time":"Varies","readme_content":"# Resume-screening-for-HR-round\n\nThe HR Interview Screening Chatbot is a cloud-based intelligent system that automates the initial stages of recruitment.  \nIt simulates a real-world hiring workflow used by companies:\n\n‚ÄúResume ‚Üí Application ‚Üí ATS Filter -> Mail\"\n---\n\"Mail‚Üí Assessment Link ‚Üí Assessment Scoring ‚Üí HR Shortlist‚Äù\n---\nThe system uses Streamlit + Python for the applicant interface, HTML and CSS and vanilla JS for assesment interface, Snowflake as the data warehouse, Azure for deployment and email automation, and Power BI for recruiter dashboards.\n\nThe goal of the project is to reduce recruiter workload and shortlist the most relevant candidates using a transparent and explainable AI scoring system.\n\n---\n\n## Key Features\n### Admin page :\n     Job decscription is available here, assesment questions depends on jd. key words are also in Jd.\n     \n### 1. Candidate Application Portal (Streamlit)\n- Candidate fills an application form.  \n- Uploads PDF resume.  \n- Stores submission in Snowflake.  \n- Shows message: ‚ÄúAssessment link will be emailed if your resume passes ATS filter.‚Äù\n\n### 2. ATS Resume Filtering (Python + NLP)\n- Keywords and skill extraction.  \n- Semantic similarity scoring.  \n- Computes ATS Score (0‚Äì100).  \n- If ATS score ‚â• threshold, an assessment link is emailed.\n\n### 3. Assessment Workflow\n- Candidate receives secure link via email.  \n- Takes the MCQ/coding assessment.  \n- System auto-grades and stores the result.  \n- Assessment score is compared with the threshold.\n\n### 4. Final Composite Scoring\nFor candidates who pass both gates, the system evaluates:\n- Resume Score  \n- Application Form Answer Score  \n- Assessment Score  \n\nAll three are combined to calculate a composite AI score.\n\n### 5. Recruiter Dashboard (Power BI)\n- Displays top candidates.  \n- Filters by ATS, Assessment, and Final Score.  \n- View resume information and summaries.  \n- Provides transparent score breakdowns.\n\n---\n\n## Why This Project Is Useful\n\nRecruiters handle thousands of applications. Manually checking resumes, distributing assessments, scoring results, and shortlisting candidates is slow.\n\nThis system automates:\n- Resume screening  \n- Assessment distribution  \n- Scoring  \n- Shortlisting  \n- Candidate ranking  \n\nIt improves fairness, consistency, and reduces workload by 70‚Äì80%.\n\n---\n\n## Process Flow\n\n### Admin page :\n     Job decscription is available here, assesment questions depends on jd. key words are also in Jd.\n\n### 1. Candidate Submission\nCandidates visit the Streamlit app and:\n- Fill personal details  \n- Upload resume (PDF only)  \n- Answer basic application questions  \n\nData is stored in Snowflake.\n\n### 2. ATS Scoring\nATS score is calculated using:\n- Extracted skills  \n- Keywords  \n- Role requirements  \n- Semantic similarity  \n\nIf ATS Score ‚â• ATS_MIN, the candidate moves forward.  \nIf not, the application ends.\n\n### 3. Assessment Link Generation\nIf ATS passes:\n- The system generates a secure assessment token  \n- Sends the assessment link to the candidate‚Äôs email  \n\n### 4. Candidate Completes Assessment\nCandidate takes the assessment, which may include:\n- MCQs  \n- Optional coding tasks  \n\n### 5. Assessment Gate\nIf Assessment Score ‚â• ASSESSMENT_MIN, the candidate proceeds to HR evaluation.  \nOtherwise, the candidate is rejected.\n\n### 6. Composite AI Scoring\nFor candidates who pass both gates:\n\nfinal_score = 0.4 * resume_score + 0.3 * application_score + 0.3 * assessment_score\n\nA detailed score breakdown is stored for transparency.\n\n### 7. Recruiter Shortlist\nThe Power BI dashboard shows:\n- Final Score  \n- ATS Score  \n- Assessment Score  \n- Resume keywords  \n- Application answers  \n\nRecruiters review only the shortlisted profiles.\n\n---\n\n## System Architecture\n\n           +-----------------------------+\n           |       Streamlit UI          |\n           | (Resume + Application Form) |\n           +-------------+---------------+\n                         |\n                         v\n           +-----------------------------+\n           |      Python Backend         |\n           |  ATS Scoring + NLP Parsing  |\n           +-------------+---------------+\n                         |\n                         v\n           +-----------------------------+\n           |          Snowflake          |\n           |  (Candidate & Score Data)   |\n           +-------------+---------------+\n                         |\n                         v\n           +-----------------------------+\n           |     Assessment Service \n           |         HTML & CSS\n           | (MCQ/Coding Auto-Grading)   |\n           +-------------+---------------+\n                         |\n                         v\n           +-----------------------------+\n           |   Composite Scoring Engine  |\n           +-------------+---------------+\n                         |\n                         v\n           +-----------------------------+\n           |      Power BI Dashboard     |\n           |     (HR Shortlist View)     |\n           +-----------------------------+\n\n\n---\n\n## Tech Stack\n\nFrontend: Streamlit, HTML, CSS\nBackend: Python (NLP for ATS scoring)  \nDatabase / DW: Snowflake    \nVisualization: Power BI  \nSoftware Tool Used:SendGrid\n\n## Project Descriptiion\n- Admin page has JD, After the candidate fills the application and the application questions consists of :(name, email, gender, ph no, city, state, country, pincode, address, applied role, work experience, languages known, highest level of education, name of university, feild of study, year of graduation, skills, resume url,CTC expected, why this company, do you have the work permit of the applied country, do you in future require any help from the company for visa permit) and then the application is submitted and the ats scoring takes place based on the keyword matching and if the ats score is equal or higher than what we set teh candidate will be get the assessment link through mail ----> python+streamlit--> stored in snowflake\n\n-after the candidate receives the mail and starts the assignment and the assignment consists of 10 qestions(MCQ)(MCQ, options, answers all stored in seperate snowflake table) and the candidate submits the assessment and the assesment should be scored and if the assesment score is higher than the given score the candidate goes under holistic screening that consists of resume screening + assesment score + application answers(application answers means CTC, work permit eligibilty is taken as base ) and if the avarage score is equal or higher than the score set then the candidate is shortlisted to present on power bi for HR ----> HTML, CSS, Vanilla JS ---> stored in snowflake\n\n-HR has a view of shortlisted candidates in table and graph of their selection reason(CTC, keyword matching kindaa)---> power bi\n\n## Database details:\nName, email, gender, phno, city, state, pincode, country, address, applied role, resume url, ats score, ats pass, assesment token, assesment score, final score, key word score, shortlisted, shortlisted score, job description, skills, work experience, languages known, highest level of education, name of university, feild of study, year of graduation \n\n","setup_steps":[],"technologies":[],"stars":0,"forks":1,"last_updated":"2025-12-20T04:07:44Z","created_at":"2025-12-09T09:17:19Z","default_branch":"main"},{"name":"Heart-disease-detection-naive-bayes-","department":"AI","title":"Heart Disease Detection Naive Bayes ","description":"No description available","repo_url":"https://github.com/Universal-college-projects/Heart-disease-detection-naive-bayes-","demo_url":null,"languages":["HTML","Python"],"topics":[],"difficulty":"beginner","estimated_time":"Varies","readme_content":"# Heart-disease-detection-naive-bayesian\nüìå Project Overview\n\nHeart disease is one of the leading causes of death worldwide. Early prediction and diagnosis can significantly reduce mortality rates and improve patient outcomes. This project focuses on the design and implementation of a Heart Disease Prediction System using the Naive Bayes algorithm.\n\nThe system analyzes patient health parameters and predicts whether a person is likely to have heart disease. It provides both command-line and web-based interfaces, making it easy to use for healthcare applications.\n\nüéØ Objectives\n\nTo predict heart disease using machine learning techniques\n\nTo implement the Naive Bayes classifier for medical data analysis\n\nTo develop a user-friendly web interface for prediction\n\nTo assist doctors and patients in early diagnosis\n\nTo achieve reliable prediction accuracy with minimal computational cost\n\nüß† Technology Stack\n\nProgramming Language: Python\n\nMachine Learning Algorithm: Naive Bayes (GaussianNB)\n\nWeb Framework: Flask\n\nLibraries Used:\n\npandas\n\nnumpy\n\nscikit-learn\n\nflask\n\nFrontend: HTML, CSS\n\nDataset: Heart disease medical dataset\n\n‚öôÔ∏è System Architecture\n\nThe system consists of the following modules:\n\nData Generation Module\n\nModel Training Module\n\nPrediction Module (CLI & Web)\n\nWeb Interface Module\n\nVisualization Module (Histogram / Graphs)\n\nüìÇ Project Structure\nHeart-Disease-Prediction/\n‚îÇ\n‚îú‚îÄ‚îÄ data/\n‚îÇ   ‚îî‚îÄ‚îÄ heart_disease_data.csv\n‚îÇ\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îú‚îÄ‚îÄ app.py\n‚îÇ   ‚îú‚îÄ‚îÄ train_model.py\n‚îÇ   ‚îú‚îÄ‚îÄ predict.py\n‚îÇ   ‚îú‚îÄ‚îÄ generate_data.py\n‚îÇ   ‚îú‚îÄ‚îÄ heart_disease_model.pkl\n‚îÇ\n‚îú‚îÄ‚îÄ templates/\n‚îÇ   ‚îî‚îÄ‚îÄ result.html\n‚îÇ\n‚îú‚îÄ‚îÄ static/\n‚îÇ   ‚îî‚îÄ‚îÄ styles.css\n‚îÇ\n‚îú‚îÄ‚îÄ verify_histogram.py\n‚îú‚îÄ‚îÄ requirements.txt\n‚îî‚îÄ‚îÄ README.md\n\nüß™ Methodology\n\nData Collection\nMedical data containing patient health attributes is collected.\n\nData Preprocessing\nData is cleaned and prepared for training.\n\nModel Training\nA Gaussian Naive Bayes classifier is trained using Scikit-learn.\n\nPrediction\nThe trained model predicts the likelihood of heart disease.\n\nResult Visualization\nPrediction confidence and population comparison are displayed graphically.\n\n‚ñ∂Ô∏è How to Run the Project\n1Ô∏è‚É£ Install Dependencies\npip install -r requirements.txt\n\n2Ô∏è‚É£ Generate Dataset (Optional)\npython src/generate_data.py\n\n\nThis creates:\n\ndata/heart_disease_data.csv\n\n3Ô∏è‚É£ Train the Model\npython src/train_model.py\n\n\nTrains the Naive Bayes model\n\nDisplays accuracy\n\nSaves the model as:\n\nsrc/heart_disease_model.pkl\n\n4Ô∏è‚É£ Run Prediction (Command Line)\npython src/predict.py\n\n\nEnter patient details when prompted\n\nGet heart disease prediction instantly\n\n5Ô∏è‚É£ Run Web Application\npython src/app.py\n\n\nOpen browser and visit:\n\nhttp://127.0.0.1:5000\n\n\nEnter health parameters\n\nView prediction result\n\nSee confidence percentage and histogram\n\nüìä Output Screens\n\nPrediction Result Page\n\nLikely Heart Disease / No Heart Disease\n\nConfidence Percentage\n\nPopulation Comparison Graph\n\n‚úÖ Advantages\n\nSimple and efficient algorithm\n\nHigh prediction accuracy\n\nLow computational complexity\n\nUser-friendly interface\n\nSuitable for real-time healthcare applications\n\n‚ö†Ô∏è Limitations\n\nDepends on quality of dataset\n\nAssumes independence between features\n\nPerformance may vary with unseen data\n\nüöÄ Future Enhancements\n\nIntegration with real-time hospital databases\n\nUse of hybrid machine learning models\n\nMobile application support\n\nCloud-based deployment\n\nIntegration with wearable health devices\n\nüìö Conclusion\n\nThis project successfully demonstrates the application of Naive Bayes classification in predicting heart disease. The system provides an effective decision-support tool for early diagnosis and healthcare assistance. With further enhancements, it can be deployed in real-world medical environments.\n\nüë©‚Äçüíª Developed By\n\nDesign and Implementing Heart Disease Prediction Using Naive Bayesian\nMajor Project ‚Äì Artificial Intelligence and Machine Learning\n","setup_steps":[],"technologies":["Python","scikit-learn","pandas","Flask","HTML","NumPy"],"stars":0,"forks":0,"last_updated":"2025-12-19T16:04:11Z","created_at":"2025-12-09T09:18:43Z","default_branch":"main"},{"name":"Automatic-traffic-signal-control-using-AI","department":"AI","title":"Automatic Traffic Signal Control Using Ai","description":"No description available","repo_url":"https://github.com/Universal-college-projects/Automatic-traffic-signal-control-using-AI","demo_url":null,"languages":["Python"],"topics":[],"difficulty":"intermediate","estimated_time":"Varies","readme_content":"# AI based smart traffic signal optimization using realtime vahicle detection\n\n\nhttps://github.com/Natnael-k/AI-based-Traffic-Control-System--\n","setup_steps":[],"technologies":["Python"],"stars":0,"forks":0,"last_updated":"2025-12-19T06:41:06Z","created_at":"2025-12-09T09:58:24Z","default_branch":"main"},{"name":"go-grocery-serve-app","department":"CSE","title":"Go Grocery Serve App","description":"No description available","repo_url":"https://github.com/Universal-college-projects/go-grocery-serve-app","demo_url":null,"languages":[],"topics":[],"difficulty":"intermediate","estimated_time":"Varies","readme_content":"# üõí Go Grocery ‚Äì Android Grocery Delivery Application\n\n## 1. Abstract\n\nGo Grocery is an Android-based grocery delivery application designed to simplify the process of purchasing daily grocery items. The app allows users to browse products by category, add items to a cart, and place orders dynamically. Firebase is used as a backend service to manage user authentication, real-time database operations, and cloud storage. The system reduces manual shopping effort and provides a scalable, secure, and efficient solution suitable for real-world and academic projects.\n\n---\n\n## 2. Working of the System\n\nThe Go Grocery application follows a client‚Äìserver architecture using Firebase as the backend:\n\n1. User registers or logs in using Firebase Authentication.\n2. After login, the user is redirected to the Home Screen.\n3. Products are fetched dynamically from Firebase Firestore.\n4. Users can browse categories such as Vegetables, Fruits, Dairy, Snacks, and Drinks.\n5. Selected products are added to the cart and stored in Firestore.\n6. During checkout, order details are saved in the Orders collection.\n7. Admin can manage products and update order status through Firebase Console.\n8. Users can view order history and profile details.\n\n---\n\n## 3. UML Diagrams (9 Diagrams)\n\n1. **Use Case Diagram** ‚Äì Shows interactions between User, Admin, and the system.\n2. **Class Diagram** ‚Äì Represents classes such as User, Product, Cart, Order, and Admin.\n3. **Sequence Diagram (Login)** ‚Äì Shows authentication flow.\n4. **Sequence Diagram (Order Placement)** ‚Äì Describes cart-to-order process.\n5. **Activity Diagram (User Flow)** ‚Äì Overall navigation flow of the app.\n6. **Activity Diagram (Admin Flow)** ‚Äì Product and order management flow.\n7. **Data Flow Diagram (DFD ‚Äì Level 0)** ‚Äì High-level system overview.\n8. **Data Flow Diagram (DFD ‚Äì Level 1)** ‚Äì Detailed data movement between modules.\n9. **Deployment Diagram** ‚Äì Shows Android app, Firebase services, and users.\n\n> üìå UML diagrams are available in the `uml_diagrams/` folder.\n\n---\n\n## 4. Results (Output Screens)\n\nThe application successfully performs all intended operations. Below are the result screens:\n\n* Splash Screen\n* Login Screen\n* Register Screen\n* Home Screen\n* Category Product Screen\n* Cart Screen\n* Checkout Screen\n* Order History Screen\n* Profile Screen\n\n> üì∏ Screenshots are available in the `results_images/` folder.\n\n---\n\n## 5. Training Images\n\nTraining images include UI reference designs and sample product images used during development:\n\n* App UI wireframes\n* Product images (Vegetables, Fruits, Dairy, Snacks)\n* Firebase database structure screenshots\n\n> üñº Images are stored in the `training_images/` folder.\n\n---\n\n## 6. Software Tools Used\n\n* Android Studio\n* Firebase Console\n* Git & GitHub\n* Google Chrome (Testing)\n* Figma (UI Design ‚Äì Optional)\n\n---\n\n## 7. Code Libraries Used\n\n### Android Libraries\n\n* androidx.appcompat\n* androidx.recyclerview\n* androidx.constraintlayout\n* com.google.android.material\n\n### Firebase Libraries\n\n* Firebase Authentication\n* Firebase Firestore\n* Firebase Storage\n* Firebase Analytics (optional)\n\n### Other Libraries\n\n* Glide (for image loading)\n* Gson (JSON parsing ‚Äì optional)\n\n---\n\n## üìå Conclusion\n\nGo Grocery provides a complete end-to-end grocery delivery solution using Android and Firebase. The project demonstrates real-time database handling, authentication, and dynamic UI updates, making it suitable for academic submission and practical learning.\n\n","setup_steps":[],"technologies":[],"stars":0,"forks":0,"last_updated":"2025-12-19T05:05:52Z","created_at":"2025-12-09T10:01:29Z","default_branch":"main"},{"name":"Skin-disease-detection-and-classification-using-cnn","department":"AI","title":"Skin Disease Detection And Classification Using Cnn","description":"No description available","repo_url":"https://github.com/Universal-college-projects/Skin-disease-detection-and-classification-using-cnn","demo_url":null,"languages":["Python","CSS","JavaScript","HTML"],"topics":[],"difficulty":"beginner","estimated_time":"Varies","readme_content":"<<<<<<< HEAD\n# Skin Disease Prediction AI\n\nThis project is a Deep Learning application designed to classify various skin diseases from images. It uses a Convolutional Neural Network (CNN) built with **TensorFlow/Keras** and provides a user-friendly **Flask** web interface for real-time predictions.\n\n## üöÄ Features\n- **High-Accuracy Classification**: Trained on a diverse dataset of skin conditions.\n- **Web Interface**: Modern, responsive web app for easy image upload and analysis.\n- **Real-Time Inference**: Fast predictions with confidence scores.\n- **CLI Support**: Command-line scripts for batch processing or quick checks.\n\n## üõ†Ô∏è Prerequisites\nEnsure you have **Python 3.8+** installed. You will need the following libraries:\n\n- TensorFlow\n- Flask\n- NumPy\n- Matplotlib (for training visualization)\n\n## üì¶ Installation\n\n1.  **Clone or Download** this repository to your local machine.\n2.  **Install Dependencies**:\n    Open your terminal or command prompt in the project directory and run:\n    ```bash\n    pip install tensorflow flask numpy matplotlib\n    ```\n\n## üìÇ Project Structure\n\n```\nskin project/\n‚îú‚îÄ‚îÄ data set/               # Dataset directory (Test/Train split)\n‚îú‚îÄ‚îÄ static/                 # CSS and JavaScript for the web app\n‚îÇ   ‚îú‚îÄ‚îÄ style.css\n‚îÇ   ‚îî‚îÄ‚îÄ script.js\n‚îú‚îÄ‚îÄ templates/              # HTML templates\n‚îÇ   ‚îî‚îÄ‚îÄ index.html\n‚îú‚îÄ‚îÄ app.py                  # Flask Web Application entry point\n‚îú‚îÄ‚îÄ train_model.py          # Script to train the CNN model\n‚îú‚îÄ‚îÄ evaluate_model.py       # Script to evaluate model performance\n‚îú‚îÄ‚îÄ predict.py              # CLI script for single image prediction\n‚îú‚îÄ‚îÄ skin_disease_model.keras # Saved trained model file\n‚îî‚îÄ‚îÄ README.md               # Project documentation\n```\n\n## üñ•Ô∏è usage\n\n### 1. Running the Web Application (Recommended)\nThis starts the local web server where you can upload images via a graphical interface.\n\n1.  Run the Flask app:\n    ```bash\n    python app.py\n    ```\n2.  Wait for the message: `Model loaded successfully`.\n3.  Open your web browser and go to:\n    ```\n    http://127.0.0.1:5000\n    ```\n4.  Drag and drop an image or click to upload, then click **\"Analyze Image\"**.\n\n### 2. Training the Model\nIf you want to retrain the model with new data:\n\n1.  Ensure your data is organized in `data set/train` and `data set/test` with subfolders for each class.\n2.  Run:\n    ```bash\n    python train_model.py\n    ```\n    This will save the new model as `skin_disease_model.keras`.\n\n### 3. Evaluating the Model\nTo check the accuracy and loss on the test dataset:\n\n```bash\npython evaluate_model.py\n```\n\n### 4. Command Line Prediction\nTo predict a single image without starting the web server:\n\n1.  Open `predict.py` and modify the image path at the bottom.\n2.  Run:\n    ```bash\n    python predict.py\n    ```\n\n## ‚ö†Ô∏è Note\n- The model expects images to be resized to **180x180** pixels (handled automatically by the scripts).\n- Ensure the `data set` folder exists if you plan to retrain or if the app needs to load class names dynamically.\n\n## ü§ù Contributing\nFeel free to fork this project and submit pull requests for improvements!\n=======\n# Skin-disease-detection-and-classification-using-cnn\n\n\nhttps://github.com/varshitha-g/Skin-Disease-Prediction\n>>>>>>> b967eb6c0c349a02ab1be9e38ecf9c4c469235c6\nupdate README\n","setup_steps":[],"technologies":["JavaScript","Python","Keras","Flask","TensorFlow","HTML","CSS","NumPy"],"stars":0,"forks":0,"last_updated":"2025-12-18T05:57:40Z","created_at":"2025-12-09T10:04:04Z","default_branch":"main"},{"name":"autonomous-drone","department":"Robotics","title":"Autonomous Drone","description":"No description available","repo_url":"https://github.com/Universal-college-projects/autonomous-drone","demo_url":null,"languages":[],"topics":[],"difficulty":"intermediate","estimated_time":"Varies","readme_content":"The project ‚ÄúAutonomous Drone using DroneKit, MAVLink, MAVProxy, MAVGen with Machine Learning Simulation‚Äù aims to build a fully automatic drone system that can fly, navigate, detect obstacles, and make decisions without human control. The drone is developed and tested in a simulation environment using machine learning.\n\nThe core idea is to use DroneKit to write Python programs for controlling the drone, MAVLink for communication, MAVGen to automatically generate MAVLink message libraries, and MAVProxy for monitoring and controlling the drone. Machine learning models help the drone recognize obstacles or objects from simulated camera input and respond intelligently. The entire system is safely tested using SITL, Gazebo, or AirSim simulation.\n\nShort Descriptions of Each Component\n1. DroneKit\n\nA Python library used to program the drone.\n\nControls takeoff, landing, and autonomous missions.\n\n2. MAVLink\n\nA lightweight communication protocol.\n\nConnects the drone autopilot with ground stations and scripts.\n\nTransfers telemetry like GPS, altitude, speed, sensor data.\n\n3. MAVGen\n\nA code generator tool inside MAVLink.\n\nAutomatically creates MAVLink message classes in languages like Python or C.\n\nHelps in customizing messages and building new drone functionalities.\n\nEnsures DroneKit and MAVProxy can understand the same message format.\n\n4. MAVProxy\n\nA Ground Control Station (GCS) software.\n\nMonitors the drone‚Äôs live data and allows command forwarding.\n\nActs as a communication hub while testing autonomous missions.\n\n5. Machine Learning + Simulation\n\nUses environments like SITL, Gazebo, or AirSim.\n\nTrains ML models for:\n\nObstacle detection\n\nObject recognition\n\nPath adjustment\n\nThe simulated drone uses these predictions to fly safely and intelligently.\n\nHow the System Works (Simple Flow)\n\nSimulation Setup:\nA virtual drone is created using SITL or AirSim.\n\nMessage Generation (MAVGen):\nMAVGen creates the necessary MAVLink message libraries.\n\nCommunication (MAVLink + MAVProxy):\nMAVProxy connects DroneKit scripts with the simulated drone using MAVLink messages.\n\nAutonomous Control (DroneKit):\nPython scripts control:\n\nArming\n\nTakeoff\n\nWaypoints\n\nMission execution\n\nLanding\n\nMachine Learning Decision Making:\nThe ML model processes camera images and helps the drone avoid obstacles or detect targets.\n\nMonitoring:\nMAVProxy shows real-time data for testing and debugging.\n","setup_steps":[],"technologies":[],"stars":0,"forks":0,"last_updated":"2025-12-18T03:56:55Z","created_at":"2025-12-09T10:05:55Z","default_branch":"main"},{"name":"road-condition-monitoring","department":"AI","title":"Road Condition Monitoring","description":"No description available","repo_url":"https://github.com/Universal-college-projects/road-condition-monitoring","demo_url":null,"languages":["Python"],"topics":[],"difficulty":"intermediate","estimated_time":"Varies","readme_content":"# Road Condition Monitoring ‚Äì AI-Based Pothole Detection\n\nThis project uses Artificial Intelligence to automatically detect road damages such as potholes from a standard camera feed. Using a YOLO-based deep learning model and OpenCV, the system analyzes each video frame to identify potholes and then generates a professional PDF report with material estimates and repair budget. The entire pipeline runs automatically on a laptop using Python.\n\n---\n\n## Features\n\n- YOLO-based pothole detection on road videos (e.g., dashcam footage).\n- Live detection window showing bounding boxes around potholes.\n- De-duplication logic so each pothole is counted only once across frames.\n- Automatic estimation per pothole:\n  - Length and width (approximate)\n  - Sand (kg), Cement (kg), Rocks (kg), Coal Tar (liters)\n  - Individual pothole repair budget\n- Aggregated ‚ÄúMaterials & Cost Summary‚Äù:\n  - Total sand, cement, rocks and coal tar required\n  - Total material costs + labour cost\n  - Final estimated repair budget\n- Professional letter-style PDF report addressed to road authorities.\n- Automatic email sending of the PDF report via Gmail.\n\n---\n\n## Project Structure (important files)\n\n```\nR_C_M/\n‚îú‚îÄ‚îÄ best.pt                     # Trained YOLO model for pothole detection\n‚îú‚îÄ‚îÄ potholes_video.mp4          # Sample input video\n‚îú‚îÄ‚îÄ auto_pothole_report.py      # Main end-to-end pipeline (detect + report + email)\n‚îú‚îÄ‚îÄ .env                        # Email credentials (not committed to Git)\n‚îú‚îÄ‚îÄ venv/                       # Python virtual environment (ignored)\n‚îú‚îÄ‚îÄ README.md\n‚îî‚îÄ‚îÄ ...                         # Other helper scripts / logs\n```\n\n---\n\n## Requirements\n\n- Python 3.10+\n- Windows (tested) with VS Code or any IDE\n- Gmail account with App Password enabled\n\nPython packages:\n\n```\nultralytics\nopencv-python\nreportlab\npython-dotenv\n```\n\n---\n\n## Setup Instructions\n\n### 1. Clone the repository\n\n```\ngit clone https://github.com/<your-username>/road-condition-monitoring.git\ncd road-condition-monitoring/R_C_M\n```\n\n### 2. Create and activate virtual environment (recommended)\n\n```\npython -m venv venv\n# Windows:\nvenv\\Scripts\\activate\n```\n\n### 3. Install dependencies\n\n```\npip install ultralytics opencv-python reportlab python-dotenv\n```\n\n### 4. Add your YOLO model and sample video\n\nPlace the following in the `R_C_M` folder:\n\n- `best.pt` ‚Äì your trained YOLOv8/YOLOv11 pothole detector.\n- `potholes_video.mp4` ‚Äì sample road inspection video.\n\n### 5. Create `.env` file for email\n\nCreate a file named `.env` in `R_C_M` (same folder as `auto_pothole_report.py`):\n\n```\nSENDER_EMAIL=your_email@gmail.com\nGMAIL_APP_PASSWORD=xxxx xxxx xxxx xxxx\n```\n\nUse a Gmail **App Password**, not your normal password (from Google Account ‚Üí Security ‚Üí 2‚ÄëStep Verification ‚Üí App passwords).\n\n### 6. Configure report details (optional)\n\nOpen `auto_pothole_report.py` and adjust these variables near the top:\n\n```\nVIDEO_PATH = \"potholes_video.mp4\"\nMODEL_PATH = \"best.pt\"\nGUIDE_EMAIL = \"road.authority@example.com\"\nGUIDE_NAME = \"Road Authority Officer\"\nROAD_NAME = \"NH-44\"\nAREA_NAME = \"Bangalore-Chennai Highway Section\"\n```\n\n---\n\n## How to Run (Single Command)\n\nFrom inside the activated virtual environment:\n\n```\ncd R_C_M\npython auto_pothole_report.py\n```\n\nThe script will:\n\n1. Open a live window showing pothole detections on the video.\n2. Automatically de-duplicate detections and estimate materials per pothole.\n3. Generate `pothole_detection_report.pdf` in the same folder.\n4. Email the PDF report to `GUIDE_EMAIL` using the credentials from `.env`.\n\n---\n\n## Output Report\n\nThe generated PDF contains:\n\n- Formal letter addressed to the specified road authority.\n- Short description of the inspected road segment.\n- Table of each pothole with:\n  - ID, length, width, detection accuracy\n  - Sand (kg), cement (kg), rocks (kg), coal tar (liters)\n  - Individual pothole repair budget\n- ‚ÄúMaterials & Cost Summary‚Äù table with total quantities and costs.\n- Final total estimated budget for repairing all detected potholes.\n```\n","setup_steps":["Open a live window showing pothole detections on the video.","Automatically de-duplicate detections and estimate materials per pothole.","Generate `pothole_detection_report.pdf` in the same folder.","Email the PDF report to `GUIDE_EMAIL` using the credentials from `.env`."],"technologies":["Python"],"stars":0,"forks":0,"last_updated":"2025-12-17T17:46:16Z","created_at":"2025-12-09T09:56:48Z","default_branch":"main"},{"name":"blockchain","department":"CSE","title":"Blockchain","description":"No description available","repo_url":"https://github.com/Universal-college-projects/blockchain","demo_url":null,"languages":["JavaScript","HTML","Solidity","CSS"],"topics":[],"difficulty":"intermediate","estimated_time":"Varies","readme_content":"# Blockchain-Based Student-Teacher-Admin Network\r\n\r\nA decentralized application (DApp) for managing academic interactions securely on the blockchain.\r\n\r\n## Technology Stack\r\n- **Blockchain**: Solidity, Ganache\r\n- **Frontend**: HTML5, CSS3, JavaScript, Web3.js\r\n- **Wallet**: MetaMask\r\n\r\n## Prerequisites\r\n1. **Google Chrome** with [MetaMask Extension](https://metamask.io/) installed.\r\n2. [Ganache](https://trufflesuite.com/ganache/) (Quickstart workspace).\r\n3. VS Code (to edit code if needed).\r\n\r\n## Setup & Deployment Steps\r\n\r\n### 1. Setup Blockchain (Ganache)\r\n1. Open **Ganache** and click **\"Quickstart\"**.\r\n2. Note the **RPC Server** URL (usually `HTTP://127.0.0.1:7545`) and the **Chain ID** (usually `1337`).\r\n\r\n### 2. Configure MetaMask\r\n1. Open MetaMask and add a new network manually:\r\n   - **Network Name**: Ganache Local\r\n   - **RPC URL**: `HTTP://127.0.0.1:7545`\r\n   - **Chain ID**: `1337`\r\n   - **Currency Symbol**: ETH\r\n2. Import an Account:\r\n   - Copy a **Private Key** from one of the accounts in Ganache.\r\n   - Initialise MetaMask -> Click Circle Icon -> Import Account -> Paste Private Key.\r\n\r\n### 3. Deploy Smart Contract (Remix IDE)\r\n1. Go to [Remix IDE](https://remix.ethereum.org/).\r\n2. Create a new file `UnifiedNetwork.sol` and copy the content from the local `UnifiedNetwork.sol` file in this folder.\r\n3. **Compile**: Go to \"Solidity Compiler\" tab -> Click \"Compile UnifiedNetwork.sol\".\r\n\r\nuse parias evm version only \r\n\r\n\r\n4. **Deploy**:\r\n   - Go to \"Deploy & Run Transactions\" tab.\r\n   - Set **Environment** to **\"Injected Provider - MetaMask\"**. (Make sure MetaMask is connected to Ganache).\r\n   - Click **\"Deploy\"**.\r\n5. **Copy Address**: After deployment, scroll down to \"Deployed Contracts\", copy the contract address (e.g., `0x123...`).\r\n\r\n### 4. Connect Frontend\r\n1. Open `app.js` in this folder.\r\n2. Replace `\"YOUR_CONTRACT_ADDRESS_HERE\"` with the address you just copied.\r\n   ```javascript\r\n   const contractAddress = \"0xYourCopiedAddress...\";\r\n   ```\r\n3. Save the file.\r\n\r\n### 5. Run the Application\r\n1. You can simply double-click `index.html` to open it in Chrome (or use Live Server extension in VS Code for better experience).\r\n2. **Login Admin**: The account used to deploy the contract is the **Admin**. Connect with that account on `index.html`.\r\n3. **Add Teacher**: Use Admin dashboard to register a new Teacher address (pick another address from Ganache).\r\n4. **Login Teacher**: Switch MetaMask account to the Teacher address, reload `index.html` and login.\r\n5. **Add Student**: Use Teacher dashboard to register a Student address.\r\n6. **Login Student**: Switch MetaMask account to Student address, reload `index.html` and login.\r\n\r\n## Troubleshooting\r\n- **\"Connection Failed\"**: Ensure Ganache is running and MetaMask is on the correct network.\r\n- **\"Contract Connection Failed\"**: Verify `contractAddress` in `app.js` matches the deployed address in Remix.\r\n\r\n\r\n\r\n16/12/2025  Final Optimizations\r\n\r\nAdmin dash board required features\r\n     - It should contain details about different departments like CSE,ECE,EEE,CIVIL,MECH,AI&ML,AI-DS,CDS and more\r\n     - Admin should give an access to students that to see notifications\r\n     - admin block should have an unique design than others\r\n\r\nTeacher dashboard required features\r\n     - access to upload Fee details update\r\n     - Access to upload (materials, notes & previous papers, belongs to different subjects) - access to paste links \r\n     - to send notifications about every information (to give updates)\r\n     - to assign results to each student\r\n     - access to prepare time table of examinations\r\n     - access to prepare time table of each department classes\r\n     - Behaviour documentation of Student\r\n     - to give info about carrer implementation about jobs realated to government and private\r\n     - to assign weekly challenges to students\r\n\r\n\r\nStudentb Dashboard required features\r\n      - to take weekly challenges \r\n      - to ask questions in AI Assiatant\r\n      - to check fee receipt & verifications\r\n      - to Check results\r\n      - to give feedback about each faculty\r\n      - emmergency communication purpose of studies,sports,projects & about issues both offical & personal\r\n      - to check carrer implemtation about jobs related to both government & private\r\n\r\n\r\n\r\n\r\n17-12-2025\r\n    - Upload login page to every Dashboard (Admin,Teacher,Student)\r\n","setup_steps":[],"technologies":["CSS","JavaScript","HTML","Solidity"],"stars":0,"forks":0,"last_updated":"2025-12-17T13:51:11Z","created_at":"2025-12-09T09:56:15Z","default_branch":"main"},{"name":"Emotion-detection-from-speech-and-facial-expression-","department":"AI","title":"Emotion Detection From Speech And Facial Expression ","description":"No description available","repo_url":"https://github.com/Universal-college-projects/Emotion-detection-from-speech-and-facial-expression-","demo_url":null,"languages":["Python","Batchfile"],"topics":[],"difficulty":"intermediate","estimated_time":"Varies","readme_content":"Emotion Detection Using Speech Recognition and Facial Expressions Developed a multimodal emotion detection system  using speech recognition and facial expression analysis, utilizing CNNs to classify emotions like joy, anger, and sadness.  Improved human-computer interaction applications in fields such as smart robotics and customer service.\r\n\r\n#Documentation\r\n[documentation.pdf](https://github.com/user-attachments/files/24210747/documentation.pdf)\r\n","setup_steps":[],"technologies":["Batchfile","Python","Express"],"stars":0,"forks":0,"last_updated":"2025-12-17T10:36:20Z","created_at":"2025-12-09T10:15:36Z","default_branch":"main"},{"name":"my-personal-chef","department":"CSE","title":"My Personal Chef","description":"No description available","repo_url":"https://github.com/Universal-college-projects/my-personal-chef","demo_url":null,"languages":[],"topics":[],"difficulty":"intermediate","estimated_time":"Varies","readme_content":"We setup the complete project structure \n## Present No need of any help\n","setup_steps":[],"technologies":[],"stars":0,"forks":0,"last_updated":"2025-12-15T19:31:46Z","created_at":"2025-12-09T10:03:11Z","default_branch":"main"},{"name":"vitamin-deficiency","department":"AI","title":"Vitamin Deficiency","description":"No description available","repo_url":"https://github.com/Universal-college-projects/vitamin-deficiency","demo_url":null,"languages":[],"topics":[],"difficulty":"intermediate","estimated_time":"20 mins","readme_content":"# Vitamin Deficiency Prediction Based on User Symptoms\n\n# ‚úÖ **Vitamin Deficiency Prediction Based on Symptoms**\n\nA **machine-learning + rule-based** health-support system that identifies possible vitamin deficiencies based on symptoms that the user enters and provides:\n\n‚úî Predicted vitamin deficiency\n‚úî Food sources to correct the deficiency\n‚úî Daily lifestyle changes\n‚úî Supplement suggestions (non-medical, general)\n‚úî Preventive tips\n\n> **Important:** This is not a medical diagnosis system; it gives educational predictions.\n\n---\n\n# üß† **How the System Works**\n\nThere are two ways to build it:\n\n---\n\n## **1Ô∏è‚É£ Rule-Based Prediction System (Simple & Highly Accurate for Symptoms)**\n\nEach symptom is mapped to possible vitamin deficiencies.\n\n### **Example Mapping**\n\n| Symptom             | Possible Deficiency       |\n| ------------------- | ------------------------- |\n| Hair loss           | Vitamin B7 (Biotin), D, E |\n| Fatigue             | B12, D, Iron              |\n| Cracked lips        | B2, B6                    |\n| Vision problems     | Vitamin A                 |\n| Weak immunity       | Vitamin C, D              |\n| Tingling hands/feet | B1, B6, B12               |\n| Bone/joint pain     | Vitamin D, Calcium        |\n| Dry skin            | A, E                      |\n| Mouth ulcers        | B9, B12                   |\n\nThe system checks all symptoms the user enters and predicts one or more deficiencies.\n\n---\n\n## **2Ô∏è‚É£ ML-Based Prediction (If You Want AI Model)**\n\n### Dataset structure:\n\n| symptom1    | symptom2     | symptom3 | ‚Ä¶       | vitamin_deficiency |\n| ----------- | ------------ | -------- | ------- | ------------------ |\n| hair loss   | fatigue      | dry skin | ‚Ä¶       | Vitamin D          |\n| mouth ulcer | cracked lips | ‚Ä¶        | Vit B12 |                    |\n\nTrain using:\n‚úî Random Forest\n‚úî SVM\n‚úî Decision Trees\n‚úî Neural networks (simple MLP)\n\n---\n\n# üß∞ **Project Workflow**\n\n1. **User enters symptoms**\n   Example: ‚Äúfatigue, hair loss, tingling feet‚Äù\n\n2. **Symptom ‚Üí Deficiency Prediction**\n   Using rule-based or ML model.\n\n3. **Generate output:**\n\n   * vitamin deficiency\n   * recommended foods\n   * lifestyle changes\n   * supplement recommendations\n   * warning when to see a doctor\n\n---\n\n# üéØ **Final Output Example**\n\n### **Input:**\n\n*‚ÄúHair fall, fatigue, tingling feet‚Äù*\n\n### **Prediction:**\n\n‚û° **Likely Vitamin B12 Deficiency**\n‚û° Possible: **Vitamin D**, **Biotin**\n\n---\n\n## ü•ó Food Suggestions\n\n### **For Vitamin B12**\n\n* Eggs\n* Milk, curd\n* Fish (salmon, tuna)\n* Lean chicken\n* Fortified cereals\n\n### **For Vitamin D**\n\n* Sunlight (morning 20 mins)\n* Mushrooms\n* Fortified milk\n* Egg yolk\n\n### **For Biotin (B7)**\n\n* Almonds\n* Walnuts\n* Sweet potatoes\n* Whole grains\n\n---\n\n## üíä Home Remedies (Non-Medical)\n\n* Include **curd + banana** for gut health\n* **1 tsp flaxseed** daily for skin & hair\n* **Moringa powder** (rich in B-complex)\n\n---\n\n## üèÉ Lifestyle Recommendations\n\n* 20‚Äì30 min morning sunlight daily\n* Reduce junk food & sugar\n* Follow regular sleep schedule (7‚Äì8 hrs)\n* Reduce stress with breathing exercises\n* Drink 2.5‚Äì3 liters of water daily\n\n---\n\n## ‚ö† ‚ÄúWhen to See a Doctor‚Äù\n\n* Long-term numbness/tingling\n* Extreme fatigue\n* Unexplained weight loss\n* Severe hair loss\n\n---\n\n# üóÇ Project Modules (for your academic submission)\n\n### **1. Symptom Input Module**\n\nUser enters symptoms via text or list checklist.\n\n### **2. Preprocessing Module**\n\nTokenize ‚Üí map symptoms ‚Üí dataset encoding.\n\n### **3. Prediction Engine**\n\n* **Rule-based JSON** mapping\n* OR **ML model.pkl** prediction\n\n### **4. Recommendation Module**\n\n* Food\n* Lifestyle\n* Remedies\n\nhttps://github.com/shivasaisanga/vitamin-deficiency-prediction-and-food-recommendation.git\n* Prevention tips\n\n### **5. Report Generation**\n\nShows final predicted deficiency with suggestions.\n\n","setup_steps":[],"technologies":["IoT"],"stars":0,"forks":0,"last_updated":"2025-12-11T18:58:14Z","created_at":"2025-12-09T10:02:42Z","default_branch":"main"},{"name":"face-detcetion-and-security-alert","department":"AI","title":"Face Detcetion And Security Alert","description":"No description available","repo_url":"https://github.com/Universal-college-projects/face-detcetion-and-security-alert","demo_url":null,"languages":["Python"],"topics":[],"difficulty":"intermediate","estimated_time":"Varies","readme_content":"# face-detcetion-and-security-alert\n\nLiveCam Face Recognition with Multi person matching and Threshold Alert\n\n\n\nimport os\nimport dlib\nimport face_recognition\nimport mediapipe as mp\nimport numpy as np\nimport cv2\nfrom FaceMetrics import FaceMetrics\n\n# Initialize Mediapipe Face Mesh\nmp_face_mesh = mp.solutions.face_mesh\nmp_drawing = mp.solutions.drawing_utils\nmp_drawing_styles = mp.solutions.drawing_styles\n\n# Set up video capture\ncap = cv2.VideoCapture(0)\n\n\nEYE_AR_THRESH = 0.25\nMOUTH_AR_THRESH = 0.98\nEYE_AR_CONSEC_imageS = 3\nCOUNTER = 5\n\nKNOWN_FACES_DIR =  \"../registered_faces\" # training data\nTOLERANCE = 0.4\nimage_THICKNESS = 1 # rectangle thickness\nFONT_THICKNESS = 2 # font thickness\nMODEL = \"cnn\" # convolutional\nprint(\"loading known faces\")\nknown_faces = [] # store known faces here\nknown_names = [] # store name of them here\nprint(dlib.DLIB_USE_CUDA) # ture, if cuda is enabled.\n### TRAIN THE CNN MODEL ON KNOWN FACES #############################\nfor name in os.listdir(KNOWN_FACES_DIR):\n    for filename in os.listdir(f\"{KNOWN_FACES_DIR}/{name}\"):\n        image = face_recognition.load_image_file(f\"{KNOWN_FACES_DIR}/{name}/{filename}\")\n        encoding = face_recognition.face_encodings(image)[0]\n        known_faces.append(encoding)\n        known_names.append(name)\n        \n\n\nwith mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, refine_landmarks=True, min_detection_confidence=0.5, min_tracking_confidence=0.5) as face_mesh:\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Convert frame to RGB\n        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    locations = face_recognition.face_locations(image,model=MODEL) # find face\n    encodings = face_recognition.face_encodings(image, locations) # extrac face features\n    for face_encoding, face_location in zip(encodings, locations):\n        if len(face_encoding) == 0:\n            continue  # Skip if encoding not found\n\n        results = face_recognition.compare_faces(known_faces, face_encoding)\n        face_dist = face_recognition.face_distance(known_faces, face_encoding)\n        print(f\"Face distances: {face_dist}\")\n\n        matchIndex = np.argmin(face_dist)\n        match = \"Unknown\"\n        if results[matchIndex] and face_dist[matchIndex]<=TOLERANCE: # if there is a known face\n            match = known_names[matchIndex] # who that person is\n            print(f\"Match found: {match}\")\n            ### DRAW RECTANGLE AND #####################################\n            top_left = (face_location[3], face_location[0])\n            bottom_right = (face_location[1], face_location[2])\n            color = [0, 255, 0] # green \n            cv2.rectangle(image, top_left, bottom_right, color, image_THICKNESS)\n            top_left = (face_location[3], face_location[2])\n            bottom_right = (face_location[1], face_location[2]+22)\n            cv2.rectangle(image, top_left, bottom_right, color, cv2.FILLED)\n            cv2.putText(image, match, (face_location[3]+10, face_location[2]+15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), FONT_THICKNESS)\n        \n        \n\n        \n        image.flags.writeable = False\n        results = face_mesh.process(image)\n\n        # Convert back to BGR for display\n        image.flags.writeable = True\n        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n\n        if results.multi_face_landmarks:\n            for face_landmarks in results.multi_face_landmarks:\n                # Get image dimensions\n                h, w, _ = image.shape\n\n                # Create FaceMetrics instance\n                landmarks = face_landmarks.landmark\n\n                # Draw face mesh\n                mp_drawing.draw_landmarks(\n                    image=image,\n                    landmark_list=face_landmarks,\n                    connections=mp_face_mesh.FACEMESH_TESSELATION,\n                    landmark_drawing_spec=None,\n                    connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style())\n\n                mp_drawing.draw_landmarks(\n                    image=image,\n                    landmark_list=face_landmarks,\n                    connections=mp_face_mesh.FACEMESH_CONTOURS,\n                    landmark_drawing_spec=None,\n                    connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_contours_style())\n\n\n\n                metrics = FaceMetrics(landmarks, w, h)\n                analysis = metrics.analyze()\n                ear = analysis['common_ear']\n                mar = analysis['mouth_aspect_ratio']\n                if ear < EYE_AR_THRESH:\n                    COUNTER += 1\n                    if COUNTER >= EYE_AR_CONSEC_imageS:\n                        cv2.putText(image, \"Eyes Closed!\", (500, 20),\n                                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n                else:\n                    COUNTER = 0\n\n                # MAR Calculation\n                \n                cv2.putText(image, \"MAR: {:.2f}\".format(mar), (20, 20),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n\n                if mar < MOUTH_AR_THRESH:\n                    cv2.putText(image, \"Yawning!\", (500, 20),\n                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n\n                # Display analysis results\n                cv2.putText(image, f\"EAR: {analysis['common_ear']:.2f}\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n                cv2.putText(image, f\"MAR: {analysis['mouth_aspect_ratio']:.2f}\", (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n                cv2.putText(image, f\"Direction: {analysis['head_direction']}\", (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n\n        # Show the frame\n        cv2.imshow(\"Face Metrics\", image)\n\n        # Exit on pressing 'q'\n        if cv2.waitKey(5) & 0xFF == ord('q'):\n            break\n\ncap.release()\ncv2.destroyAllWindows()\n","setup_steps":[],"technologies":["NumPy","Python"],"stars":0,"forks":0,"last_updated":"2025-12-11T09:10:14Z","created_at":"2025-12-09T09:52:50Z","default_branch":"main"},{"name":"Multilingual-chatbot-rasa","department":"AI","title":"Multilingual Chatbot Rasa","description":"No description available","repo_url":"https://github.com/Universal-college-projects/Multilingual-chatbot-rasa","demo_url":null,"languages":[],"topics":[],"difficulty":"intermediate","estimated_time":"Varies","readme_content":"# Multilingual-chatbot-rasa\nhttps://github.com/suryadevelope/rasa_chatbot_multilang_medical\n","setup_steps":[],"technologies":[],"stars":0,"forks":0,"last_updated":"2025-12-10T05:36:12Z","created_at":"2025-12-09T09:30:38Z","default_branch":"main"},{"name":"Breast-cancer-detection-using-cnn-","department":"AI","title":"Breast Cancer Detection Using Cnn ","description":"No description available","repo_url":"https://github.com/Universal-college-projects/Breast-cancer-detection-using-cnn-","demo_url":null,"languages":[],"topics":[],"difficulty":"intermediate","estimated_time":"Varies","readme_content":"# Breast-cancer-detection-using-cnn-\nhttps://github.com/123Naseema/BREAST-CANCER-DETECTION-USING-CNN-AND-RNN-/blob/main/README.md\n","setup_steps":[],"technologies":[],"stars":0,"forks":0,"last_updated":"2025-12-09T17:29:12Z","created_at":"2025-12-09T09:17:48Z","default_branch":"main"},{"name":"Genetic-diseases-","department":"AI","title":"Genetic Diseases ","description":"No description available","repo_url":"https://github.com/Universal-college-projects/Genetic-diseases-","demo_url":null,"languages":[],"topics":[],"difficulty":"intermediate","estimated_time":"Varies","readme_content":"\nhttps://github.com/Anandpaul99s/Automatic-Detection-of-Genetic-Diseases-in-Pediatric-Age-Using-Pupillometry/tree/main\n","setup_steps":[],"technologies":[],"stars":0,"forks":0,"last_updated":"2025-12-09T17:23:12Z","created_at":"2025-12-09T09:21:15Z","default_branch":"main"},{"name":"Fine-tuning-llm","department":"AI","title":"Fine Tuning Llm","description":"No description available","repo_url":"https://github.com/Universal-college-projects/Fine-tuning-llm","demo_url":null,"languages":[],"topics":[],"difficulty":"intermediate","estimated_time":"Varies","readme_content":"# Fine-tuning-llm\nhttps://github.com/poloclub/Fine-tuning-LLMs/tree/main\n","setup_steps":[],"technologies":[],"stars":0,"forks":0,"last_updated":"2025-12-09T14:11:58Z","created_at":"2025-12-09T09:31:00Z","default_branch":"main"}];
    const allTechs = new Set();
    let totalStars = 0;

    projectsData.forEach(project => {
        (project.languages || []).forEach(lang => allTechs.add(lang));
        (project.technologies || []).forEach(tech => allTechs.add(tech));
        totalStars += project.stars || 0;
    });

    // Update stats
    document.getElementById('totalLanguages').textContent = allTechs.size;
    document.getElementById('totalStars').textContent = totalStars;

    // Add technology filter buttons
    const techFiltersContainer = document.getElementById('techFilters');
    const sortedTechs = Array.from(allTechs).sort();

    sortedTechs.forEach(tech => {
        const btn = document.createElement('button');
        btn.className = 'filter-chip';
        btn.setAttribute('data-filter', tech.toLowerCase());
        btn.textContent = tech;
        techFiltersContainer.appendChild(btn);
    });

    // Filter functionality
    let currentDifficulty = 'all';
    let currentTech = 'all';
    let searchQuery = '';

    function filterProjects() {
        const cards = document.querySelectorAll('.project-card');
        let visibleCount = 0;

        cards.forEach(card => {
            const difficulty = card.getAttribute('data-difficulty');
            const languages = (card.getAttribute('data-languages') || '').toLowerCase();
            const technologies = (card.getAttribute('data-technologies') || '').toLowerCase();
            const name = card.getAttribute('data-name');
            const description = card.getAttribute('data-description');

            const matchesDifficulty = currentDifficulty === 'all' || difficulty === currentDifficulty;
            const matchesTech = currentTech === 'all' ||
                languages.includes(currentTech) ||
                technologies.includes(currentTech);
            const matchesSearch = searchQuery === '' ||
                name.includes(searchQuery) ||
                description.includes(searchQuery);

            if (matchesDifficulty && matchesTech && matchesSearch) {
                card.style.display = '';
                visibleCount++;
            } else {
                card.style.display = 'none';
            }
        });

        // Show/hide no results message
        const noResults = document.getElementById('noResults');
        if (visibleCount === 0) {
            noResults.classList.remove('hidden');
        } else {
            noResults.classList.add('hidden');
        }
    }

    // Difficulty filter
    document.getElementById('difficultyFilters').addEventListener('click', (e) => {
        if (e.target.classList.contains('filter-chip')) {
            document.querySelectorAll('#difficultyFilters .filter-chip').forEach(btn => {
                btn.classList.remove('active');
            });
            e.target.classList.add('active');
            currentDifficulty = e.target.getAttribute('data-filter');
            filterProjects();
        }
    });

    // Technology filter
    document.getElementById('techFilters').addEventListener('click', (e) => {
        if (e.target.classList.contains('filter-chip')) {
            document.querySelectorAll('#techFilters .filter-chip').forEach(btn => {
                btn.classList.remove('active');
            });
            e.target.classList.add('active');
            currentTech = e.target.getAttribute('data-filter');
            filterProjects();
        }
    });

    // Search
    document.getElementById('searchInput').addEventListener('input', (e) => {
        searchQuery = e.target.value.toLowerCase();
        filterProjects();
    });
</script>
  </main>

  <footer>
    &copy; 2025 GK Solutions ‚Ä¢ Built with ‚ù§Ô∏è on GitHub Pages
</footer>

<script src="/assets/js/main.js"></script>
</body>

</html>